

<!doctype html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Welcome to ml-indie-tools’s documentation! &#8212; ml-indie-tools 0.3.8 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/bizstyle.css" />
    
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/bizstyle.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="A collection of machine learning tools for low-resource research and experiments" href="README.html" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0" />
    <!--[if lt IE 9]>
    <script src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="README.html" title="A collection of machine learning tools for low-resource research and experiments"
             accesskey="N">next</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">ml-indie-tools 0.3.8 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Welcome to ml-indie-tools’s documentation!</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="welcome-to-ml-indie-tools-s-documentation">
<h1>Welcome to ml-indie-tools’s documentation!<a class="headerlink" href="#welcome-to-ml-indie-tools-s-documentation" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<section id="module-env_tools">
<span id="mlenv-object"></span><h2>MLEnv object<a class="headerlink" href="#module-env_tools" title="Permalink to this headline">¶</a></h2>
<p>Tools to configure ML environment for Tensorflow, Pytorch or JAX and
optional notebook/colab environment</p>
<dl class="py class">
<dt class="sig sig-object py" id="env_tools.MLEnv">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">env_tools.</span></span><span class="sig-name descname"><span class="pre">MLEnv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">platform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'tf'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accelerator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'fastest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">old_disable_eager</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/env_tools.html#MLEnv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#env_tools.MLEnv" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Initialize platform and accelerator.</p>
<p>This checks initialization and available accelerator hardware for different ml platforms.
At return, the following variables are set: <cite>self.is_tensorflow</cite>, <cite>self.is_pytorch</cite>, <cite>self.is_jax</cite>,
indicating that the ml environment is available for Tensorflow, Pytorch or JAX respectively if <cite>True</cite>.
<cite>self.is_notebook</cite> and <cite>self.is_colab</cite> indicate if the environment is a notebook or colab environment.
<cite>self.is_gpu</cite> indicates if the environment is a GPU environment, <cite>self.is_tpu</cite> indicates if the
environment is a TPU environment, and <cite>self.is_cpu</cite> that no accelerator is available.</p>
<p>The logger <cite>MLEnv</cite> provdides details about the hardware and ml environment.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>platform</strong> – Known platforms are: <cite>‘tf’</cite> (tensorflow), <cite>‘pt’</cite> (pytorch), and <cite>‘jax’</cite></p></li>
<li><p><strong>accelerator</strong> – known accelerators are: <cite>‘fastest’</cite> (pick best available hardware), <cite>‘cpu’</cite>, <cite>‘gpu’</cite>, <cite>‘tpu’</cite>.</p></li>
<li><p><strong>old_disable_eager</strong> – default ‘False’, on True, old v1 compatibility layer is used to disable eager mode.</p></li>
</ul>
</dd>
</dl>
<p>According to rumors that might in resulting old codepaths being used?</p>
<dl class="py method">
<dt class="sig sig-object py" id="env_tools.MLEnv.describe">
<span class="sig-name descname"><span class="pre">describe</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/env_tools.html#MLEnv.describe"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#env_tools.MLEnv.describe" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints a description of the machine environment.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>str: description of the machine environment.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="env_tools.MLEnv.describe_mlenv">
<span class="sig-name descname"><span class="pre">describe_mlenv</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/env_tools.html#MLEnv.describe_mlenv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#env_tools.MLEnv.describe_mlenv" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="env_tools.MLEnv.describe_osenv">
<span class="sig-name descname"><span class="pre">describe_osenv</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/env_tools.html#MLEnv.describe_osenv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#env_tools.MLEnv.describe_osenv" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.gpu_type">
<span class="sig-name descname"><span class="pre">gpu_type</span></span><a class="headerlink" href="#env_tools.MLEnv.gpu_type" title="Permalink to this definition">¶</a></dt>
<dd><p>GPU type, e.g. <cite>‘Tesla V100’</cite></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="env_tools.MLEnv.init_paths">
<span class="sig-name descname"><span class="pre">init_paths</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">project_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/env_tools.html#MLEnv.init_paths"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#env_tools.MLEnv.init_paths" title="Permalink to this definition">¶</a></dt>
<dd><p>Initializes the paths for the project.</p>
<p>Depending on if this is a Colab environment or not, persistent data will be stored in either
<cite>project_path=’/content/drive/My Drive/Colab Notebooks/&lt;project_name&gt;’</cite> or <cite>project_path=’.’</cite>.</p>
<p>If Google drive access is not available, data will be stored in <cite>project_path=’.’</cite>. This data
is lost, once the Colab session is closed.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">project_path</span><span class="o">/</span><span class="n">data</span>  <span class="c1"># training data (cache)</span>
<span class="n">project_path</span><span class="o">/</span><span class="n">model</span><span class="p">[</span><span class="o">/&lt;</span><span class="n">model_name</span><span class="o">&gt;</span><span class="p">]</span>  <span class="c1"># model state, weights, etc.</span>
<span class="o">.</span><span class="n">logs</span>  <span class="c1"># log files</span>
</pre></div>
</div>
<p>Note that log_path is always local, since Colab Google drive caching prevents useful logs to Google drive.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>project_name</strong> – name of the project. Only used for Colab environments. Is always current directory for non-Colab environments.</p></li>
<li><p><strong>model_name</strong> – name of the model. Optional name for model subdirectory to allow support for multiple models.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(root_path, project_path, model_path, data_path, log_path)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.is_colab">
<span class="sig-name descname"><span class="pre">is_colab</span></span><a class="headerlink" href="#env_tools.MLEnv.is_colab" title="Permalink to this definition">¶</a></dt>
<dd><p><cite>True</cite> if running in a colab notebook</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.is_conda">
<span class="sig-name descname"><span class="pre">is_conda</span></span><a class="headerlink" href="#env_tools.MLEnv.is_conda" title="Permalink to this definition">¶</a></dt>
<dd><p><cite>True</cite> if running in a conda environment</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.is_cpu">
<span class="sig-name descname"><span class="pre">is_cpu</span></span><a class="headerlink" href="#env_tools.MLEnv.is_cpu" title="Permalink to this definition">¶</a></dt>
<dd><p><cite>True</cite> if no accelerator is available</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.is_gpu">
<span class="sig-name descname"><span class="pre">is_gpu</span></span><a class="headerlink" href="#env_tools.MLEnv.is_gpu" title="Permalink to this definition">¶</a></dt>
<dd><p><cite>True</cite> if a GPU is is available</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.is_jax">
<span class="sig-name descname"><span class="pre">is_jax</span></span><a class="headerlink" href="#env_tools.MLEnv.is_jax" title="Permalink to this definition">¶</a></dt>
<dd><p><cite>True</cite> if running on Jax</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.is_notebook">
<span class="sig-name descname"><span class="pre">is_notebook</span></span><a class="headerlink" href="#env_tools.MLEnv.is_notebook" title="Permalink to this definition">¶</a></dt>
<dd><p><cite>True</cite> if running in a notebook</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.is_pytorch">
<span class="sig-name descname"><span class="pre">is_pytorch</span></span><a class="headerlink" href="#env_tools.MLEnv.is_pytorch" title="Permalink to this definition">¶</a></dt>
<dd><p><cite>True</cite> if running on Pytorch</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.is_tensorflow">
<span class="sig-name descname"><span class="pre">is_tensorflow</span></span><a class="headerlink" href="#env_tools.MLEnv.is_tensorflow" title="Permalink to this definition">¶</a></dt>
<dd><p><cite>True</cite> if running on Tensorflow</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.is_tpu">
<span class="sig-name descname"><span class="pre">is_tpu</span></span><a class="headerlink" href="#env_tools.MLEnv.is_tpu" title="Permalink to this definition">¶</a></dt>
<dd><p><cite>True</cite> if a TPU is is available</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.jax_version">
<span class="sig-name descname"><span class="pre">jax_version</span></span><a class="headerlink" href="#env_tools.MLEnv.jax_version" title="Permalink to this definition">¶</a></dt>
<dd><p>Jax version, e.g. <cite>‘0.1.0’</cite></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="env_tools.MLEnv.mount_gdrive">
<span class="sig-name descname"><span class="pre">mount_gdrive</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mount_point</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'/content/drive'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">root_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'/content/drive/My</span> <span class="pre">Drive'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/env_tools.html#MLEnv.mount_gdrive"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#env_tools.MLEnv.mount_gdrive" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.os_type">
<span class="sig-name descname"><span class="pre">os_type</span></span><a class="headerlink" href="#env_tools.MLEnv.os_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Operating system type, e.g. <cite>‘Linux’</cite>, <cite>‘Darwin’</cite></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.pt_version">
<span class="sig-name descname"><span class="pre">pt_version</span></span><a class="headerlink" href="#env_tools.MLEnv.pt_version" title="Permalink to this definition">¶</a></dt>
<dd><p>Pytorch version, e.g. <cite>‘1.6.0’</cite></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.py_version">
<span class="sig-name descname"><span class="pre">py_version</span></span><a class="headerlink" href="#env_tools.MLEnv.py_version" title="Permalink to this definition">¶</a></dt>
<dd><p>Python version, e.g. <cite>‘3.7.3’</cite></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.tf_version">
<span class="sig-name descname"><span class="pre">tf_version</span></span><a class="headerlink" href="#env_tools.MLEnv.tf_version" title="Permalink to this definition">¶</a></dt>
<dd><p>Tensorflow version, e.g. <cite>‘2.7.0’</cite></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="env_tools.MLEnv.tpu_type">
<span class="sig-name descname"><span class="pre">tpu_type</span></span><a class="headerlink" href="#env_tools.MLEnv.tpu_type" title="Permalink to this definition">¶</a></dt>
<dd><p>TPU type, e.g. <cite>‘TPU v2’</cite></p>
</dd></dl>

</dd></dl>

</section>
<section id="module-tuner">
<span id="mltuner-object"></span><h2>MLTuner object<a class="headerlink" href="#module-tuner" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="tuner.MLTuner">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tuner.</span></span><span class="sig-name descname"><span class="pre">MLTuner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">search_space</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">progress_callback</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tuner.html#MLTuner"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tuner.MLTuner" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Simple hyper parameter tuner</p>
<p>Sample <cite>search_space</cite>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="n">param_space_minimal_prm</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;dense_layers&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span>
    <span class="s2">&quot;dense_neurons&quot;</span><span class="p">:[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">768</span><span class="p">],</span> 
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.002</span><span class="p">],</span>
    <span class="s2">&quot;regu1&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-7</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>search_space</strong> – Dictionary defining the search space.</p></li>
<li><p><strong>progress_callback</strong> – Callback function that is called after each iteration with updated search space as parameter.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="tuner.MLTuner.tune">
<span class="sig-name descname"><span class="pre">tune</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param_space</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_func</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/tuner.html#MLTuner.tune"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tuner.MLTuner.tune" title="Permalink to this definition">¶</a></dt>
<dd><p>Tune hyper parameters</p>
<p>Example parameter space:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_space</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;dense_layers&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span>
    <span class="s2">&quot;dense_neurons&quot;</span><span class="p">:[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">768</span><span class="p">],</span> 
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.002</span><span class="p">],</span>
    <span class="s2">&quot;regu1&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-7</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p><cite>eval_func</cite> is called with a dictionary of hyper parameters with exactly one value for each key, e.g.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="n">params</span><span class="o">=</span><span class="p">{</span>
    <span class="s2">&quot;dense_layers&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="s2">&quot;dense_neurons&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span> 
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
    <span class="s2">&quot;regu1&quot;</span><span class="p">:</span> <span class="mf">1e-8</span>
<span class="p">}</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>param_space</strong> – Dictionary defining the search space.</p></li>
<li><p><strong>eval_func</strong> – Function that is called to evaluate the hyper parameters.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-Gutenberg_Dataset">
<span id="gutenberg-dataset-object"></span><h2>Gutenberg_Dataset object<a class="headerlink" href="#module-Gutenberg_Dataset" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="Gutenberg_Dataset.Gutenberg_Dataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">Gutenberg_Dataset.</span></span><span class="sig-name descname"><span class="pre">Gutenberg_Dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">root_url</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'https://www.gutenberg.org/dirs'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gutenberg'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Gutenberg_Dataset.html#Gutenberg_Dataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Gutenberg_Dataset.Gutenberg_Dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A fuzzy, lightweight class to access, search and filter Project Gutenberg resources</p>
<p>GutenbergLib by default uses a mirror’s root URL. Alternatively, you can specify a local directory containing a Gutenberg mirror.
That mirror directory needs to contain a GUTINDEX.ALL file and has typically many
sub-directories <cite>0</cite> ,.. <cite>n</cite> .</p>
<p>A mirror of project Gutenberg can be created by:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>!/bin/bash
<span class="go">rsync -zarv --dry-run --prune-empty-dirs --del --include=&quot;*/&quot; --include=&#39;*.&#39;{txt,pdf,ALL} --exclude=&quot;*&quot; aleph.gutenberg.org::gutenberg ./gutenberg_mirror</span>
</pre></div>
</div>
<p>You can remove the PDF files, since they are currently not used, and need to review the <cite>–dry-run</cite> option.</p>
<p>Note: <a class="reference internal" href="#Gutenberg_Dataset.Gutenberg_Dataset.load_index" title="Gutenberg_Dataset.Gutenberg_Dataset.load_index"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_index()</span></code></a> needs to be called before any other methods.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root_url</strong> – url of Project Gutenberg or any mirror URL, or a local directory containing a Gutenberg mirror.</p></li>
<li><p><strong>cache_dir</strong> – path to a directory that will be used to cache the Gutenberg index and already downloaded texts.</p></li>
</ul>
</dd>
</dl>
<p>The cache directory is only used, if a remote Gutenberg URL and not a local mirror is used.</p>
<dl class="py method">
<dt class="sig sig-object py" id="Gutenberg_Dataset.Gutenberg_Dataset.filter_text">
<span class="sig-name descname"><span class="pre">filter_text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">book_text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_start_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_near_start_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_end_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Gutenberg_Dataset.html#Gutenberg_Dataset.filter_text"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Gutenberg_Dataset.Gutenberg_Dataset.filter_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Heuristically remove header and trailer texts not part of the actual books</p>
<p>Unfortunatelly, formatting of Gutenberg books is an unbelievable mess. Using lists of tokens <cite>self.start_tokens</cite> (indicating
the start of the actual book text), <cite>self.near_start_tokens</cite> (indicating possibly ambiguous tokens near a <cite>start_tokens</cite> token,
further narrowing the start of text), and <cite>self.end_tokens</cite> (indicating the end of the book text), this function tries to find
the start and end of the book text. The user can either extend the lists of class member tokens, of provide temporary additional
tokens as parameter to this function.</p>
<p>The list of <cite>start_tokens</cite> contains only tokens that are always significant as being part of header-cruft (e.g. ‘START OF THIS GUTENBERG’).
<cite>near_start_tokens</cite> are tokens that might be ambiguous, but are still part of the header-cruft, (e.g. ‘produced by’).
<cite>near_start_tokens</cite> are only used, if they are within <cite>self.NEAR</cite> bytes to the latest <cite>start_tokens</cite> token,
to heuristically prevent false positives.</p>
<p><em>Note:</em> Use logging via <cite>logging.basicConfig(level=logging.DEBUG)</cite> to analyze the filtering process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>book_text</strong> – text of the book (string)</p></li>
<li><p><strong>add_start_tokens</strong> – additional start tokens (list of strings)</p></li>
<li><p><strong>add_near_start_tokens</strong> – additional near start tokens (list of strings)</p></li>
<li><p><strong>add_end_tokens</strong> – additional end tokens (list of strings)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>filtered text (string)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Gutenberg_Dataset.Gutenberg_Dataset.find_keywords">
<span class="sig-name descname"><span class="pre">find_keywords</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">search_keys</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Gutenberg_Dataset.html#Gutenberg_Dataset.find_keywords"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Gutenberg_Dataset.Gutenberg_Dataset.find_keywords" title="Permalink to this definition">¶</a></dt>
<dd><p>Search of an arbitrary number of keywords in a book record</p>
<p><em>Note:</em> <a class="reference internal" href="#Gutenberg_Dataset.Gutenberg_Dataset.load_index" title="Gutenberg_Dataset.Gutenberg_Dataset.load_index"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_index()</span></code></a> needs to be called once before this function can be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of records that contain all keywords in any field.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Gutenberg_Dataset.Gutenberg_Dataset.get_book">
<span class="sig-name descname"><span class="pre">get_book</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ebook_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Gutenberg_Dataset.html#Gutenberg_Dataset.get_book"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Gutenberg_Dataset.Gutenberg_Dataset.get_book" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a book record metadata and filtered text by its ebook_id</p>
<p>This function returns a dictionary with metadata and filtered text. Use <a class="reference internal" href="#Gutenberg_Dataset.Gutenberg_Dataset.load_book" title="Gutenberg_Dataset.Gutenberg_Dataset.load_book"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_book()</span></code></a>
to get the raw unfiltered text.</p>
<p><em>Note:</em> <a class="reference internal" href="#Gutenberg_Dataset.Gutenberg_Dataset.load_index" title="Gutenberg_Dataset.Gutenberg_Dataset.load_index"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_index()</span></code></a> needs to be called once before this function can be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>ebook_id</strong> – ebook_id (String, since some IDs contain letters) of the book to be retrieved</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>book record (dictionary with metadata and filtered text)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Gutenberg_Dataset.Gutenberg_Dataset.get_record_keys">
<span class="sig-name descname"><span class="pre">get_record_keys</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/Gutenberg_Dataset.html#Gutenberg_Dataset.get_record_keys"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Gutenberg_Dataset.Gutenberg_Dataset.get_record_keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a list of all keys that are used within records.
Standard keys are: <cite>ebook_id</cite>, <cite>author</cite>, <cite>language</cite>, <cite>title</cite>.</p>
<p><em>Note:</em> <a class="reference internal" href="#Gutenberg_Dataset.Gutenberg_Dataset.load_index" title="Gutenberg_Dataset.Gutenberg_Dataset.load_index"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_index()</span></code></a> needs to be called once before this function can be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of all different keys that are somehow used.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Gutenberg_Dataset.Gutenberg_Dataset.get_unique_record_values">
<span class="sig-name descname"><span class="pre">get_unique_record_values</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Gutenberg_Dataset.html#Gutenberg_Dataset.get_unique_record_values"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Gutenberg_Dataset.Gutenberg_Dataset.get_unique_record_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a list of all unique values a given keys has for all records.</p>
<p><em>Note:</em> <a class="reference internal" href="#Gutenberg_Dataset.Gutenberg_Dataset.load_index" title="Gutenberg_Dataset.Gutenberg_Dataset.load_index"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_index()</span></code></a> needs to be called once before this function can be used.</p>
<p>Example: <cite>get_unique_records_values(‘language’)</cite> returns all languages in Gutenberg.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>key</strong> – key to search for.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of all unique values for a given key.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Gutenberg_Dataset.Gutenberg_Dataset.insert_book_texts">
<span class="sig-name descname"><span class="pre">insert_book_texts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">search_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">download_count_limit</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Gutenberg_Dataset.html#Gutenberg_Dataset.insert_book_texts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Gutenberg_Dataset.Gutenberg_Dataset.insert_book_texts" title="Permalink to this definition">¶</a></dt>
<dd><p>Inserts book texts into the records returned by <a class="reference internal" href="#Gutenberg_Dataset.Gutenberg_Dataset.search" title="Gutenberg_Dataset.Gutenberg_Dataset.search"><code class="xref py py-func docutils literal notranslate"><span class="pre">search()</span></code></a>.</p>
<p>In order to prevent the download of too many books, the download count limit is set to <cite>download_count_limit</cite>.
Downloaded books are cached and cached books are not counted towards the download count limit. Calling this
function again will download books that have not been downloaded yet. The filtered book content is inserted
into the dictionary with the key <cite>text</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>search_dict</strong> – search array of dictionaries that at least contain the key <cite>ebook_id</cite>.</p></li>
<li><p><strong>download_count_limit</strong> – maximum number of books to download, if no local mirror is used. No limits apply for local mirrors.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of records including filtered book text-based in the <cite>text</cite> field.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Gutenberg_Dataset.Gutenberg_Dataset.load_book">
<span class="sig-name descname"><span class="pre">load_book</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ebook_id</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Gutenberg_Dataset.html#Gutenberg_Dataset.load_book"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Gutenberg_Dataset.Gutenberg_Dataset.load_book" title="Permalink to this definition">¶</a></dt>
<dd><p>get text of an ebook from Gutenberg by ebook_id</p>
<p>This function returns the unfiltered raw text including all Gutenberg headers and footers.
Use <a class="reference internal" href="#Gutenberg_Dataset.Gutenberg_Dataset.get_book" title="Gutenberg_Dataset.Gutenberg_Dataset.get_book"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_book()</span></code></a> to retrieve a dictionary with metadata and filtered text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>ebook_id</strong> – Gutenberg id (Note: string, since this sometimes contains a character!)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>book text as string, unfiltered. Can be filtered with <a class="reference internal" href="#Gutenberg_Dataset.Gutenberg_Dataset.filter_text" title="Gutenberg_Dataset.Gutenberg_Dataset.filter_text"><code class="xref py py-func docutils literal notranslate"><span class="pre">filter_text()</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Gutenberg_Dataset.Gutenberg_Dataset.load_index">
<span class="sig-name descname"><span class="pre">load_index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_expire_days</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Gutenberg_Dataset.html#Gutenberg_Dataset.load_index"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Gutenberg_Dataset.Gutenberg_Dataset.load_index" title="Permalink to this definition">¶</a></dt>
<dd><p>This function loads the Gutenberg record index, either from cache, or from a website</p>
<p>This should be the first method being used, since many other methods rely on the index being loaded.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cache</strong> – default <cite>True</cite>, use the cache directory to cache both index and text files.</p>
</dd>
</dl>
<p>Index expires after <cite>cache_expire_days</cite>, text files never expire.
Should <em>NOT</em> be set to <cite>False</cite> in order to prevent unnecessary re-downloading.
:param cache_expire_days: Number of days after which the index is re-downloaded.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Gutenberg_Dataset.Gutenberg_Dataset.search">
<span class="sig-name descname"><span class="pre">search</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">search_dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Gutenberg_Dataset.html#Gutenberg_Dataset.search"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Gutenberg_Dataset.Gutenberg_Dataset.search" title="Permalink to this definition">¶</a></dt>
<dd><p>Search for book record with key specific key values
For a list of valid keys, use <cite>get_record_keys()</cite>
Standard keys are: <cite>ebook_id</cite>, <cite>author</cite>, <cite>language</cite>, <cite>title</cite></p>
<p><em>Note:</em> <a class="reference internal" href="#Gutenberg_Dataset.Gutenberg_Dataset.load_index" title="Gutenberg_Dataset.Gutenberg_Dataset.load_index"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_index()</span></code></a> needs to be called once before this function can be used.</p>
<p>Example: <cite>search({“title”: [“philosoph”,”phenomen”,”physic”,”hermeneu”,”logic”], “language”:”english”})</cite>
Find all books whose titles contain at least one of the keywords, language english. Search keys can either be
search for a single keyword (e.g. english), or an array of keywords.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of records</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-Calibre_Dataset">
<span id="calibre-dataset-object"></span><h2>Calibre_Dataset object<a class="headerlink" href="#module-Calibre_Dataset" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="Calibre_Dataset.Calibre_Dataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">Calibre_Dataset.</span></span><span class="sig-name descname"><span class="pre">Calibre_Dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">library_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Calibre_Dataset.html#Calibre_Dataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Calibre_Dataset.Calibre_Dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A class to access and search text documents from a Calibre library.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>library_path</strong> – Path to the Calibre library</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="Calibre_Dataset.Calibre_Dataset.load_index">
<span class="sig-name descname"><span class="pre">load_index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_aliases</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Calibre_Dataset.html#Calibre_Dataset.load_index"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Calibre_Dataset.Calibre_Dataset.load_index" title="Permalink to this definition">¶</a></dt>
<dd><p>This function loads the Calibre library records that contain text-format books.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_aliases</strong> – If True, books are not referenced by title and author,</p>
</dd>
</dl>
<p>but by their numeric aliases, thus providing privacy.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Calibre_Dataset.Calibre_Dataset.search">
<span class="sig-name descname"><span class="pre">search</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">search_dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Calibre_Dataset.html#Calibre_Dataset.search"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Calibre_Dataset.Calibre_Dataset.search" title="Permalink to this definition">¶</a></dt>
<dd><p>Search for book record with key specific key values
For a list of valid keys, use <cite>get_record_keys()</cite>
Standard keys are: <cite>ebook_id</cite>, <cite>author</cite>, <cite>language</cite>, <cite>title</cite></p>
<p><em>Note:</em> <a class="reference internal" href="#Calibre_Dataset.Calibre_Dataset.load_index" title="Calibre_Dataset.Calibre_Dataset.load_index"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_index()</span></code></a> needs to be called once before this function can be used.</p>
<p>Example: <cite>search({“title”: [“philosoph”,”phenomen”,”physic”,”hermeneu”,”logic”], “language”:”english”})</cite>
Find all books whose titles contain at least one of the keywords, language english. Search keys can either be
search for a single keyword (e.g. english), or an array of keywords.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of records</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-Folder_Dataset">
<span id="folder-dataset-object"></span><h2>Folder_Dataset object<a class="headerlink" href="#module-Folder_Dataset" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="Folder_Dataset.Folder_Dataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">Folder_Dataset.</span></span><span class="sig-name descname"><span class="pre">Folder_Dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">folder_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_language</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Folder_Dataset.html#Folder_Dataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Folder_Dataset.Folder_Dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A class to access and search text documents from a folder.</p>
<p>This loads the text files from a folder and creates some metadata from the filename.
The filename format is expected to be: “Title - Author - Language.txt”</p>
<p>the ‘ - Language’ part is optional, if a default_language is provided.
For English, use default_language=’English’.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>folder_path</strong> – Path to a folder containing text files (.txt)</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="Folder_Dataset.Folder_Dataset.load_index">
<span class="sig-name descname"><span class="pre">load_index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_aliases</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Folder_Dataset.html#Folder_Dataset.load_index"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Folder_Dataset.Folder_Dataset.load_index" title="Permalink to this definition">¶</a></dt>
<dd><p>This function loads the text files from the folder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_aliases</strong> – If True, documents are not referenced by filename (containing title and author),</p>
</dd>
</dl>
<p>but by their numeric aliases, thus providing privacy.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Folder_Dataset.Folder_Dataset.search">
<span class="sig-name descname"><span class="pre">search</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">search_dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Folder_Dataset.html#Folder_Dataset.search"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Folder_Dataset.Folder_Dataset.search" title="Permalink to this definition">¶</a></dt>
<dd><p>Search for book record with key specific key values
For a list of valid keys, use <cite>get_record_keys()</cite>
Standard keys are: <cite>ebook_id</cite>, <cite>author</cite>, <cite>language</cite>, <cite>title</cite></p>
<p><em>Note:</em> <a class="reference internal" href="#Folder_Dataset.Folder_Dataset.load_index" title="Folder_Dataset.Folder_Dataset.load_index"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_index()</span></code></a> needs to be called once before this function can be used.</p>
<p>Example: <cite>search({“title”: [“philosoph”,”phenomen”,”physic”,”hermeneu”,”logic”], “language”:”english”})</cite>
Find all books whose titles contain at least one of the keywords, language english. Search keys can either be
search for a single keyword (e.g. english), or an array of keywords.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of records</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-Text_Dataset">
<span id="text-dataset-object"></span><h2>Text_Dataset object<a class="headerlink" href="#module-Text_Dataset" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="Text_Dataset.Text_Dataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">Text_Dataset.</span></span><span class="sig-name descname"><span class="pre">Text_Dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sanitize_white_space</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">separate_punctuation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preserve_case</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Text_Dataset.html#Text_Dataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Text_Dataset.Text_Dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Initialize the Text_Dataset with a list of texts.</p>
<p>The Gutenberg_Dataset can be used to create such a list, by:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ml_indie_tools.Gutenberg_Dataset</span> <span class="kn">import</span> <span class="n">Gutenberg_Dataset</span>
<span class="kn">from</span> <span class="nn">ml_indie_tools.Text_Dataset</span> <span class="kn">import</span> <span class="n">Text_Dataset</span>
<span class="n">gd</span> <span class="o">=</span> <span class="n">Gutenberg_Dataset</span><span class="p">()</span>
<span class="n">gd</span><span class="o">.</span><span class="n">load_index</span><span class="p">()</span>
<span class="n">ls</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">search</span><span class="p">({</span><span class="s1">&#39;author&#39;</span><span class="p">:</span> <span class="s1">&#39;kant&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="s1">&#39;kritik&#39;</span><span class="p">,</span> <span class="s1">&#39;language&#39;</span><span class="p">:</span> <span class="s1">&#39;german&#39;</span><span class="p">})</span>  <span class="c1"># returns a list of texts</span>
<span class="n">ls</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">insert_texts</span><span class="p">(</span><span class="n">ls</span><span class="p">)</span>  <span class="c1"># this inserts the actual text of the books into field &#39;text&#39;.</span>
<span class="c1"># Now ls contains a valid list of text records:</span>
<span class="n">td</span> <span class="o">=</span> <span class="n">Text_Dataset</span><span class="p">(</span><span class="n">ls</span><span class="p">)</span>
</pre></div>
</div>
<p>If text_list at initialization is None, texts can be added later with the load_texts() method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>text_list</strong> – optional list of text-records of the form: {‘author’: ‘author’, ‘title’: ‘title’, ‘language’: ‘some-language’,</p>
</dd>
</dl>
<p>‘text’: ‘the-long-text’}. Optional parameters: ‘weight’: 1.0
:param sanitize_white_space: If True, white space is replaced by a single space.
:param separate_punctuation: If True, punctuation is separated from words.
:param preserve_case: If True, the case of the text is preserved.</p>
<dl class="py method">
<dt class="sig sig-object py" id="Text_Dataset.Text_Dataset.decode">
<span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoded</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mark_separator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Text_Dataset.html#Text_Dataset.decode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Text_Dataset.Text_Dataset.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode a list of encoded tokens.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoded</strong> – list of encoded tokens</p></li>
<li><p><strong>mark_separator</strong> – (ngram only) if True, add a separator between tokens for debug</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>text</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Text_Dataset.Text_Dataset.encode">
<span class="sig-name descname"><span class="pre">encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Text_Dataset.html#Text_Dataset.encode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Text_Dataset.Text_Dataset.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Encode a text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>text</strong> – text to encode</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of encoded tokens</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Text_Dataset.Text_Dataset.filter_text">
<span class="sig-name descname"><span class="pre">filter_text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sanitize_white_space</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">separate_punctuation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preserve_case</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Text_Dataset.html#Text_Dataset.filter_text"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Text_Dataset.Text_Dataset.filter_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter a text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> – text to filter</p></li>
<li><p><strong>sanitize_white_space</strong> – If True, white space is replaced by a single space.</p></li>
<li><p><strong>separate_punctuation</strong> – If True, punctuation is separated from words.</p></li>
<li><p><strong>preserve_case</strong> – If True, the case of the text is preserved.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>filtered text</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Text_Dataset.Text_Dataset.get_random_item">
<span class="sig-name descname"><span class="pre">get_random_item</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/Text_Dataset.html#Text_Dataset.get_random_item"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Text_Dataset.Text_Dataset.get_random_item" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a random sample from the dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Text_Dataset.Text_Dataset.get_unique_token_count">
<span class="sig-name descname"><span class="pre">get_unique_token_count</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/Text_Dataset.html#Text_Dataset.get_unique_token_count"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Text_Dataset.Text_Dataset.get_unique_token_count" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the number of unique tokens.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>number of unique tokens</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Text_Dataset.Text_Dataset.init_getitem">
<span class="sig-name descname"><span class="pre">init_getitem</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'text'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">80</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">content_stepping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Text_Dataset.html#Text_Dataset.init_getitem"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Text_Dataset.Text_Dataset.init_getitem" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the __getitem__ and __len__ methods.</p>
<p>This method needs to be called before using len() or index-access of the dataset.</p>
<p>This method determines how the dataset is partitioned into records, and what kind
of encoding is returned on index-access.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ml_indie_tools.Text_Dataset</span> <span class="kn">import</span> <span class="n">TextDataset</span>
<span class="n">tl</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;author&#39;</span><span class="p">:</span><span class="s1">&#39;nobody&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">:</span><span class="s1">&#39;some title&#39;</span><span class="p">,</span> <span class="s1">&#39;language&#39;</span><span class="p">:</span><span class="s1">&#39;english&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span><span class="s1">&#39;some text&#39;</span><span class="p">},</span>
      <span class="p">{</span><span class="s1">&#39;author&#39;</span><span class="p">:</span><span class="s1">&#39;nobody&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">:</span><span class="s1">&#39;some title 2&#39;</span><span class="p">,</span> <span class="s1">&#39;language&#39;</span><span class="p">:</span><span class="s1">&#39;english&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span><span class="s1">&#39;some more text&#39;</span><span class="p">}]</span>
<span class="n">td</span> <span class="o">=</span> <span class="n">Text_Dataset</span><span class="p">(</span><span class="n">tl</span><span class="p">)</span>
<span class="n">td</span><span class="o">.</span><span class="n">init_getitem</span><span class="p">(</span><span class="n">sample_type</span><span class="o">=</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="n">sample_length</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">content_stepping</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">td</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">td</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># Output: 12 and (&#39;some&#39;, &#39;ome &#39;)</span>
</pre></div>
</div>
<p>The method of tokenization (char, word, ngram) for ‘encoded’ sample_type is determined by
the tokenizer_type in call to init_tokenizer().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sample_type</strong> – ‘text’ (text-string of length sample_length), ‘encoded’ (encoded sample_length tokens)</p></li>
<li><p><strong>sample_length</strong> – length of a sample (either character count (type text) or token count (type encoded))</p></li>
<li><p><strong>content_stepping</strong> – number of characters/tokens to skip between each sample</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Text_Dataset.Text_Dataset.init_tokenizer">
<span class="sig-name descname"><span class="pre">init_tokenizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ngram'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_ngrams</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_separator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Text_Dataset.html#Text_Dataset.init_tokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Text_Dataset.Text_Dataset.init_tokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the tokenizer with the text_list.</p>
<p>The character tokenizer simply splits any language text into glyphs.</p>
<p>A word tokenizer works for languages that have the concept of word separators (as in English,
‘ ‘ space). Word tokenizers are English-centric algorithms, since many other languages either
can compose words into larger words with no separators, words being more similar to English
syllables in that case, or have no word separators at all (as in Chinese), or have syllable
separators (as in Tibetan, which has also a second kind of separators for sentence-fragments). For multi-language applications, use ngram tokenizers or character
tokenizers.</p>
<p>The ngram tokenizer can either first split text into words using a word_speparator
and then extracting all possible ngrams, or it can extract all possible ngrams directly
without any word-splitting, which is the default behavior with word_separator=None since it
works for all languages classes.</p>
<p>For ngrams, the tokenizer can extract ngrams of length 1..max_ngrams, and selects the top most
used ngrams with upper limit max_tokens. The optimum for max_ngrams is usually around 3-6, and
max_tokens should be significantly higher than the number of unique glyphs in the text_list.
Using word_separator=None is usually significantly better than using a word_separator for ngrams.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tokenizer</strong> – ‘word’, ‘char’, or ‘ngram’ (default)</p></li>
<li><p><strong>max_ngrams</strong> – (ngram only) maximum n-gram length</p></li>
<li><p><strong>word_separator</strong> – (word, ngram) character used to separate words, default None, which amounts to ‘ ‘ (space) for word and no word-splitting for ngram.</p></li>
<li><p><strong>max_tokens</strong> – (ngram only) maximum number of tokens to use</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Text_Dataset.Text_Dataset.load_texts">
<span class="sig-name descname"><span class="pre">load_texts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sanitize_white_space</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">separate_punctuation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preserve_case</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Text_Dataset.html#Text_Dataset.load_texts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Text_Dataset.Text_Dataset.load_texts" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a list of texts into the Text_Dataset.</p>
<p>Note: if there are already texts in the Text_Dataset, the new texts are appended.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>text_list</strong> – list of text-records of the form: {‘author’: ‘author’, ‘title’: ‘title’, ‘language’: ‘some-language’,</p>
</dd>
</dl>
<p>‘text’: ‘the-long-text’}. Optional parameters: ‘weight’: 1.0
:param sanitize_white_space: If True, white space is replaced by a single space.
:param separate_punctuation: If True, punctuation is separated from words.
:param preserve_case: If True, the case of the text is preserved.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Text_Dataset.Text_Dataset.load_tokenizer">
<span class="sig-name descname"><span class="pre">load_tokenizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Text_Dataset.html#Text_Dataset.load_tokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Text_Dataset.Text_Dataset.load_tokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Load tokenizer data and text library from JSON-file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>file_path</strong> – path to file</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Text_Dataset.Text_Dataset.save_tokenizer">
<span class="sig-name descname"><span class="pre">save_tokenizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Text_Dataset.html#Text_Dataset.save_tokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Text_Dataset.Text_Dataset.save_tokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Save tokenizer data and text library to JSON-file.</p>
<p>This json file can be loaded with load_tokenizer() and has all information needed
to use the tokenizer with trained models and new text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>file_path</strong> – path to file</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Text_Dataset.Text_Dataset.source_highlight">
<span class="sig-name descname"><span class="pre">source_highlight</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ref_txt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_quote_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dark_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">display_ref_anchor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Text_Dataset.html#Text_Dataset.source_highlight"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Text_Dataset.Text_Dataset.source_highlight" title="Permalink to this definition">¶</a></dt>
<dd><p>Analyse which parts of <cite>ref_txt</cite> are cited from the texts in the Text_Dataset.</p>
<p>Note: this function requires a jupyter notebook in order to display HTML with markup.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ref_txt</strong> – the reference text to be analysed for plagiarised parts</p></li>
<li><p><strong>min_quote_size</strong> – minimum size of a quote to be considered plagiarised</p></li>
<li><p><strong>dark_mode</strong> – if True, the background colors will be dark, otherwise white</p></li>
<li><p><strong>display_ref_anchor</strong> – if True, the reference text will be displayed with a reference anchor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Text_Dataset.Text_Dataset.tokenize">
<span class="sig-name descname"><span class="pre">tokenize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Text_Dataset.html#Text_Dataset.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#Text_Dataset.Text_Dataset.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize a text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>text</strong> – text to tokenize</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of tokens</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-ALU_Dataset">
<span id="alu-dataset-object"></span><h2>ALU_Dataset object<a class="headerlink" href="#module-ALU_Dataset" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ALU_Dataset.ALU_Dataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ALU_Dataset.</span></span><span class="sig-name descname"><span class="pre">ALU_Dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">bit_count</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">31</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ALU_Dataset.html#ALU_Dataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ALU_Dataset.ALU_Dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Generate training data for all ALU operations</p>
<p>The ALU takes two integers and applies one of the supported
model_ops. E.g. <cite>op1=123, op2=100, op=’-’ -&gt; result 23</cite></p>
<p>The net is supposed to learn to ‘calculate’ the results for
arbitrary op1, op2 (positive integers, <cite>0..2**bit_count - 1</cite>) and
the twelve supported ops:
<cite>[“+”, “-”, “*”, “/”, “%”, “AND”, “OR”, “XOR”, “&gt;”, “&lt;”, “=”, “!=”]</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>bit_count</strong> – number of bits for each of the two operands, default 31 (mult uses 15 bits)</p></li>
<li><p><strong>pre_weight</strong> – if True, the model_dis will be reweighted to generate samples for ‘difficult’ ops</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ALU_Dataset.ALU_Dataset.check_results">
<span class="sig-name descname"><span class="pre">check_results</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vector</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positional_encoding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_ops</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ALU_Dataset.html#ALU_Dataset.check_results"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ALU_Dataset.ALU_Dataset.check_results" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a number of tests on trained model</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ALU_Dataset.ALU_Dataset.create_data_point">
<span class="sig-name descname"><span class="pre">create_data_point</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">op1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op_string</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vector</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positional_suffix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ALU_Dataset.html#ALU_Dataset.create_data_point"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ALU_Dataset.ALU_Dataset.create_data_point" title="Permalink to this definition">¶</a></dt>
<dd><p>create training data from given ints op1, op2 and op_string</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ALU_Dataset.ALU_Dataset.create_training_data">
<span class="sig-name descname"><span class="pre">create_training_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_ops</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">equal_distrib</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ALU_Dataset.html#ALU_Dataset.create_training_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ALU_Dataset.ALU_Dataset.create_training_data" title="Permalink to this definition">¶</a></dt>
<dd><p>create a number of training samples</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ALU_Dataset.ALU_Dataset.create_vector_training_data">
<span class="sig-name descname"><span class="pre">create_vector_training_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_ops</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">equal_distrib</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positional_encoding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ALU_Dataset.html#ALU_Dataset.create_vector_training_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ALU_Dataset.ALU_Dataset.create_vector_training_data" title="Permalink to this definition">¶</a></dt>
<dd><p>create a number of training samples</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ALU_Dataset.ALU_Dataset.decode_results">
<span class="sig-name descname"><span class="pre">decode_results</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">result_int_vects</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ALU_Dataset.html#ALU_Dataset.decode_results"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ALU_Dataset.ALU_Dataset.decode_results" title="Permalink to this definition">¶</a></dt>
<dd><p>take an array of 32-float results from neural net and convert to ints</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ALU_Dataset.ALU_Dataset.get_data_point">
<span class="sig-name descname"><span class="pre">get_data_point</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">equal_distrib</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_ops</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vector</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positional_encoding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ALU_Dataset.html#ALU_Dataset.get_data_point"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ALU_Dataset.ALU_Dataset.get_data_point" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a random example for on ALU operation for training</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>equal_distrib</strong> – if False, more ‘difficult’ ops will be generated more often.</p></li>
<li><p><strong>valid_ops</strong> – if not None, only the ops in valid_ops will be used</p></li>
<li><p><strong>vector</strong> – if True, the result will be returned as an embedded encoded vector</p></li>
<li><p><strong>positional_encoding</strong> – if True, the result will be returned as an embedded encoded vector with additional bits for positional positional_encoding</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ALU_Dataset.ALU_Dataset.get_datasets">
<span class="sig-name descname"><span class="pre">get_datasets</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pre_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vector</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positional_encoding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_ops</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regenerate_cached_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ALU_Dataset.html#ALU_Dataset.get_datasets"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ALU_Dataset.ALU_Dataset.get_datasets" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ALU_Dataset.ALU_Dataset.op_string_to_index">
<span class="sig-name descname"><span class="pre">op_string_to_index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">op_string</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ALU_Dataset.html#ALU_Dataset.op_string_to_index"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ALU_Dataset.ALU_Dataset.op_string_to_index" title="Permalink to this definition">¶</a></dt>
<dd><p>transform op_string (e.g. ‘+’ -&gt; 0) into corresponding index</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>op_string</strong> – string of op to transform</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>index of op_string</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="residualblock-object">
<h2>ResidualBlock object<a class="headerlink" href="#residualblock-object" title="Permalink to this headline">¶</a></h2>
</section>
<section id="residualdense-object">
<h2>ResidualDense object<a class="headerlink" href="#residualdense-object" title="Permalink to this headline">¶</a></h2>
</section>
<section id="residualdensestack-object">
<h2>ResidualDenseStack object<a class="headerlink" href="#residualdensestack-object" title="Permalink to this headline">¶</a></h2>
</section>
<section id="parallelresidualdensestacks-object">
<h2>ParallelResidualDenseStacks object<a class="headerlink" href="#parallelresidualdensestacks-object" title="Permalink to this headline">¶</a></h2>
</section>
<section id="selfattention-object">
<h2>SelfAttention object<a class="headerlink" href="#selfattention-object" title="Permalink to this headline">¶</a></h2>
</section>
<section id="multiheadselfattention-object">
<h2>MultiHeadSelfAttention object<a class="headerlink" href="#multiheadselfattention-object" title="Permalink to this headline">¶</a></h2>
</section>
<section id="module-pytorch_custom_layers">
<span id="positionalencoding-object"></span><h2>PositionalEncoding object<a class="headerlink" href="#module-pytorch_custom_layers" title="Permalink to this headline">¶</a></h2>
</section>
<section id="selfattentionhead">
<h2>SelfAttentionHead<a class="headerlink" href="#selfattentionhead" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pytorch_custom_layers.SelfAttentionHead">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pytorch_custom_layers.</span></span><span class="sig-name descname"><span class="pre">SelfAttentionHead</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embedding_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sequence_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pytorch_custom_layers.html#SelfAttentionHead"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_custom_layers.SelfAttentionHead" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Single head self-attention, optionally with causal masking.
taken from <a class="reference external" href="https://github.com/karpathy/ng-video-lecture">https://github.com/karpathy/ng-video-lecture</a>,
the explanation of nano-gpt</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embedding_size</strong> – the size of the input embedding</p></li>
<li><p><strong>sequence_len</strong> – the length of the input sequence</p></li>
<li><p><strong>dropout</strong> – the dropout rate</p></li>
<li><p><strong>head_size</strong> – the size of the attention head</p></li>
<li><p><strong>causal</strong> – whether to use causal masking</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="multiheadattention">
<h2>MultiHeadAttention<a class="headerlink" href="#multiheadattention" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pytorch_custom_layers.MultiHeadAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pytorch_custom_layers.</span></span><span class="sig-name descname"><span class="pre">MultiHeadAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embedding_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sequence_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pytorch_custom_layers.html#MultiHeadAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_custom_layers.MultiHeadAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Multiple heads of self-attention in parallel</p>
<p>Note: the embedding size must be divisible by the number of heads</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embedding_size</strong> – the size of the input embedding</p></li>
<li><p><strong>sequence_len</strong> – the length of the input sequence</p></li>
<li><p><strong>dropout</strong> – the dropout rate</p></li>
<li><p><strong>num_heads</strong> – the number of attention heads</p></li>
<li><p><strong>head_size</strong> – the size of the attention head</p></li>
<li><p><strong>causal</strong> – whether to use causal masking</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="feedfoward">
<h2>FeedFoward<a class="headerlink" href="#feedfoward" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pytorch_custom_layers.FeedFoward">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pytorch_custom_layers.</span></span><span class="sig-name descname"><span class="pre">FeedFoward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_linearity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pytorch_custom_layers.html#FeedFoward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_custom_layers.FeedFoward" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Dual linear layers separated by a non-linearity</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – the size of the input embedding</p></li>
<li><p><strong>hidden_size</strong> – the size of the ‘hidden’ linear layer in the feed-forward network,</p></li>
</ul>
</dd>
</dl>
<p>if None, use default input_size*4
:param dropout: the dropout rate (default None, don’t use dropout layer)
:param non_linearity: the non-linearity to use, one of “relu” (default), “leaky_relu”, “tanh”</p>
</dd></dl>

</section>
<section id="block">
<h2>Block<a class="headerlink" href="#block" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pytorch_custom_layers.Block">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pytorch_custom_layers.</span></span><span class="sig-name descname"><span class="pre">Block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embedding_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sequence_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_hidden_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_non_linearity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_residual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pytorch_custom_layers.html#Block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_custom_layers.Block" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Transformer block: communication followed by computation</p>
<p>Note: the embedding size must be divisible by the number of heads</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embedding_size</strong> – the size of the input embedding</p></li>
<li><p><strong>sequence_len</strong> – the length of the input sequence</p></li>
<li><p><strong>dropout</strong> – the dropout rate</p></li>
<li><p><strong>num_heads</strong> – the number of attention heads</p></li>
<li><p><strong>causal</strong> – whether to use causal masking</p></li>
<li><p><strong>linear_hidden_size</strong> – the size of hidden layer in the dual-linear layer</p></li>
</ul>
</dd>
</dl>
<p>of the feed-forward network, if None, use embedding_size*4
:param linear_non_linearity: the non-linearity to use in between the dual-linear layer,
one of “relu” (default), “leaky_relu”, “tanh”
:param linear_residual: whether to use linear residual connection, default True</p>
</dd></dl>

</section>
<section id="multiheadselfattention">
<h2>MultiHeadSelfAttention<a class="headerlink" href="#multiheadselfattention" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pytorch_custom_layers.MultiHeadSelfAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pytorch_custom_layers.</span></span><span class="sig-name descname"><span class="pre">MultiHeadSelfAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sequence_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">causal</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_hidden_sizes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_non_linearity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_residual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pytorch_custom_layers.html#MultiHeadSelfAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pytorch_custom_layers.MultiHeadSelfAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>MultiHeadSelfAttention transformer model (Karpathy nanoGPT derivative)</p>
<p>Note: the embedding size must be divisible by the number of heads</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – the size of the vocabulary</p></li>
<li><p><strong>embedding_size</strong> – the size of the input embedding</p></li>
<li><p><strong>sequence_len</strong> – the length of the input sequence</p></li>
<li><p><strong>dropout</strong> – the dropout rate</p></li>
<li><p><strong>num_heads</strong> – the number of attention heads</p></li>
<li><p><strong>num_layers</strong> – the number of transformer blocks</p></li>
<li><p><strong>causal</strong> – whether to use causal masking</p></li>
<li><p><strong>linear_hidden_sizes</strong> – array of number of neurons for the dual-linear layers</p></li>
</ul>
</dd>
</dl>
<p>respective hidden sizes (or None for default 4*embedding_size), dimension must be num_layers,
if not None.
:param linear_non_linearity: the non-linearity to use in between the dual-linear layers,
:param linear_residual: whether to use linear residual connection, default True (False is only active, if hidden_sizes[i] != embedding_size*4)
:param device: the device to use for training</p>
</dd></dl>

</section>
<section id="feedforwardwithcompression">
<h2>FeedForwardWithCompression<a class="headerlink" href="#feedforwardwithcompression" title="Permalink to this headline">¶</a></h2>
</section>
<section id="feedforwardwithcompressionstate">
<h2>FeedForwardWithCompressionState<a class="headerlink" href="#feedforwardwithcompressionstate" title="Permalink to this headline">¶</a></h2>
</section>
<section id="blockwithcompression">
<h2>BlockWithCompression<a class="headerlink" href="#blockwithcompression" title="Permalink to this headline">¶</a></h2>
</section>
<section id="blockwithcompressionstate">
<h2>BlockWithCompressionState<a class="headerlink" href="#blockwithcompressionstate" title="Permalink to this headline">¶</a></h2>
</section>
<section id="blockwithcompressionnoyokeresidual">
<h2>BlockWithCompressionNoYokeResidual<a class="headerlink" href="#blockwithcompressionnoyokeresidual" title="Permalink to this headline">¶</a></h2>
</section>
<section id="blockwithcompressionstatenoyokeresidual">
<h2>BlockWithCompressionStateNoYokeResidual<a class="headerlink" href="#blockwithcompressionstatenoyokeresidual" title="Permalink to this headline">¶</a></h2>
</section>
<section id="multiheadselfattentionwithcompression">
<h2>MultiHeadSelfAttentionWithCompression<a class="headerlink" href="#multiheadselfattentionwithcompression" title="Permalink to this headline">¶</a></h2>
</section>
<section id="multiheadselfattentionwithcompressionstate">
<h2>MultiHeadSelfAttentionWithCompressionState<a class="headerlink" href="#multiheadselfattentionwithcompressionstate" title="Permalink to this headline">¶</a></h2>
</section>
<section id="readme-file">
<h2>Readme File<a class="headerlink" href="#readme-file" title="Permalink to this headline">¶</a></h2>
</section>
</section>
<section id="a-collection-of-machine-learning-tools-for-low-resource-research-and-experiments">
<h1>A collection of machine learning tools for low-resource research and experiments<a class="headerlink" href="#a-collection-of-machine-learning-tools-for-low-resource-research-and-experiments" title="Permalink to this headline">¶</a></h1>
<a class="reference external image-reference" href="LICENSE"><img alt="License" src="http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat" /></a>
<a class="reference external image-reference" href="https://domschl.github.io/ml-indie-tools/index.html"><img alt="Docs" src="https://img.shields.io/badge/docs-stable-blue.svg" /></a>
<a class="reference external image-reference" href="https://pypi.python.org/pypi/ml-indie-tools/"><img alt="PyPI version fury.io" src="https://badge.fury.io/py/ml-indie-tools.svg" /></a>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>ml-indie-tools
</pre></div>
</div>
<p>This module contains of a collection of tools useable for researchers with limited access to compute-resources and
who change between laptop, Colab-instances and local workstations with a graphics card.</p>
<p><code class="docutils literal notranslate"><span class="pre">env_tools</span></code> checks the current environment, and populates a number of flags that allow identification of run-time
environment and available accelerator hardware. For Colab instances, it provides tools to mount Google Drive for
persistent data- and model-storage.</p>
<p>The usage scenarios are:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Env</p></th>
<th class="head"><p>Tensorflow TPU</p></th>
<th class="head"><p>Tensorflow GPU</p></th>
<th class="head"><p>Pytorch TPU</p></th>
<th class="head"><p>Pytorch GPU</p></th>
<th class="head"><p>Jax TPU</p></th>
<th class="head"><p>Jax GPU</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Colab</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p>/</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>Workstation with Nvidia</p></td>
<td><p>/</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p>/</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p>/</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>Apple Silicon</p></td>
<td><p>/</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p>/</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p>/</p></td>
<td><p>/</p></td>
</tr>
</tbody>
</table>
<p>(<cite>+</cite>: supported, <cite>/</cite>: not supported)</p>
<p><code class="docutils literal notranslate"><span class="pre">Gutenberg_Dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">Text_Dataset</span></code> are NLP libraries that provide text data and can be used in conjuction
with Huggingface <a class="reference external" href="https://huggingface.co/docs/datasets/">Datasets</a> or directly with ML libraries.</p>
<p><code class="docutils literal notranslate"><span class="pre">ALU_Dataset</span></code> is a toy-dataset that allows training of integer arithmetic and logical (ALU) operations.</p>
<p>A collection of tools that allow moving machine learning projects between local hardware and colab instances.</p>
<p>Local laptop:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ml_indie_tools.env_tools</span> <span class="kn">import</span> <span class="n">MLEnv</span>
<span class="n">ml_env</span> <span class="o">=</span> <span class="n">MLEnv</span><span class="p">(</span><span class="n">platform</span><span class="o">=</span><span class="s1">&#39;tf&#39;</span><span class="p">,</span> <span class="n">accelator</span><span class="o">=</span><span class="s1">&#39;fastest&#39;</span><span class="p">)</span>
<span class="n">ml_env</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>  <span class="c1"># -&gt; &#39;OS: Darwin, Python: 3.9.9 (Conda) Tensorflow: 2.7.0, GPU: METAL&#39;</span>
<span class="n">ml_env</span><span class="o">.</span><span class="n">is_gpu</span>   <span class="c1"># -&gt; True</span>
<span class="n">ml_env</span><span class="o">.</span><span class="n">is_tensorflow</span>  <span class="c1"># -&gt; True</span>
<span class="n">ml_env</span><span class="o">.</span><span class="n">gpu_type</span>  <span class="c1"># -&gt; &#39;METAL&#39;</span>
</pre></div>
</div>
<p>Colab instance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># !pip install -U ml_indie_tools</span>
<span class="kn">from</span> <span class="nn">ml_indie_tools.env_tools</span> <span class="kn">import</span> <span class="n">MLEnv</span>
<span class="n">ml_env</span> <span class="o">=</span> <span class="n">MLEnv</span><span class="p">(</span><span class="n">platform</span><span class="o">=</span><span class="s1">&#39;tf&#39;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s1">&#39;fastest&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ml_env</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ml_env</span><span class="o">.</span><span class="n">gpu_type</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DEBUG</span><span class="p">:</span><span class="n">MLEnv</span><span class="p">:</span><span class="n">Tensorflow</span> <span class="n">version</span><span class="p">:</span> <span class="mf">2.7.0</span>
<span class="n">DEBUG</span><span class="p">:</span><span class="n">MLEnv</span><span class="p">:</span><span class="n">GPU</span> <span class="n">available</span>
<span class="n">DEBUG</span><span class="p">:</span><span class="n">MLEnv</span><span class="p">:</span><span class="n">You</span> <span class="n">are</span> <span class="n">on</span> <span class="n">a</span> <span class="n">Jupyter</span> <span class="n">instance</span><span class="o">.</span>
<span class="n">DEBUG</span><span class="p">:</span><span class="n">MLEnv</span><span class="p">:</span><span class="n">You</span> <span class="n">are</span> <span class="n">on</span> <span class="n">a</span> <span class="n">Colab</span> <span class="n">instance</span><span class="o">.</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">MLEnv</span><span class="p">:</span><span class="n">OS</span><span class="p">:</span> <span class="n">Linux</span><span class="p">,</span> <span class="n">Python</span><span class="p">:</span> <span class="mf">3.7.12</span><span class="p">,</span> <span class="n">Colab</span> <span class="n">Jupyter</span> <span class="n">Notebook</span> <span class="n">Tensorflow</span><span class="p">:</span> <span class="mf">2.7.0</span><span class="p">,</span> <span class="n">GPU</span><span class="p">:</span> <span class="n">Tesla</span> <span class="n">K80</span>
<span class="n">The</span> <span class="n">tensorboard</span> <span class="n">extension</span> <span class="ow">is</span> <span class="n">already</span> <span class="n">loaded</span><span class="o">.</span> <span class="n">To</span> <span class="n">reload</span> <span class="n">it</span><span class="p">,</span> <span class="n">use</span><span class="p">:</span>
  <span class="o">%</span><span class="n">reload_ext</span> <span class="n">tensorboard</span>
<span class="n">OS</span><span class="p">:</span> <span class="n">Linux</span><span class="p">,</span> <span class="n">Python</span><span class="p">:</span> <span class="mf">3.7.12</span><span class="p">,</span> <span class="n">Colab</span> <span class="n">Jupyter</span> <span class="n">Notebook</span> <span class="n">Tensorflow</span><span class="p">:</span> <span class="mf">2.7.0</span><span class="p">,</span> <span class="n">GPU</span><span class="p">:</span> <span class="n">Tesla</span> <span class="n">K80</span>
<span class="n">Tesla</span> <span class="n">K80</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">ml_env.init_paths('my_project',</span> <span class="pre">'my_model')</span></code> will give a list of paths that are adapted for local and colab usage</p>
<p>Local project:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ml_env</span><span class="o">.</span><span class="n">init_paths</span><span class="p">(</span><span class="s2">&quot;my_project&quot;</span><span class="p">,</span> <span class="s2">&quot;my_model&quot;</span><span class="p">)</span>
<span class="c1"># -&gt; (&#39;.&#39;, &#39;.&#39;, &#39;./model/my_model&#39;, &#39;./data&#39;, &#39;./logs&#39;)</span>
</pre></div>
</div>
<p>The list contains <code class="docutils literal notranslate"><span class="pre">&lt;root-path&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;project-path&gt;</span></code> (both are <code class="docutils literal notranslate"><span class="pre">.</span></code>, the current directory for local projects), <code class="docutils literal notranslate"><span class="pre">&lt;model-path&gt;</span></code> to save model and weights, <code class="docutils literal notranslate"><span class="pre">&lt;data-path&gt;</span></code> for
training data and <code class="docutils literal notranslate"><span class="pre">&lt;log-path&gt;</span></code> for logs.</p>
<p>Those paths (with exception of <code class="docutils literal notranslate"><span class="pre">./logs</span></code>) are moved to Google Drive for Colab instances:</p>
<p>On Google Colab:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># INFO:MLEnv:You will now be asked to authenticate Google Drive access in order to store training data (cache) and model state.</span>
<span class="c1"># INFO:MLEnv:Changes will only happen within Google Drive directory `My Drive/Colab Notebooks/&lt;project-name&gt;`.</span>
<span class="c1"># DEBUG:MLEnv:Root path: /content/drive/My Drive</span>
<span class="c1"># Mounted at /content/drive</span>
<span class="p">(</span><span class="s1">&#39;/content/drive/My Drive&#39;</span><span class="p">,</span>
 <span class="s1">&#39;/content/drive/My Drive/Colab Notebooks/my_project&#39;</span><span class="p">,</span>
 <span class="s1">&#39;/content/drive/My Drive/Colab Notebooks/my_project/model/my_model&#39;</span><span class="p">,</span>
 <span class="s1">&#39;/content/drive/My Drive/Colab Notebooks/my_project/data&#39;</span><span class="p">,</span>
 <span class="s1">&#39;./logs&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>See the <a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#module-env_tools">env_tools API documentation</a> for details.</p>
<p>Gutenberg_Dataset makes books from <a class="reference external" href="https://www.gutenberg.org">Project Gutenberg</a> available as dataset.</p>
<p>This module can either work with a local mirror of Project Gutenberg, or download files on demand.
Files that are downloaded are cached to prevent unnecessary load on Gutenberg’s servers.</p>
<p>If you plan to use a lot of files (hundreds or more) from Gutenberg, a local mirror might be the best
solution. Have a look at <a class="reference external" href="https://www.gutenberg.org/help/mirroring.html">Project Gutenberg’s notes on mirrors</a>.</p>
<p>A mirror image suitable for this project can be made with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>rsync<span class="w"> </span>-zarv<span class="w"> </span>--dry-run<span class="w"> </span>--prune-empty-dirs<span class="w"> </span>--del<span class="w"> </span>--include<span class="o">=</span><span class="s2">&quot;*/&quot;</span><span class="w"> </span>--include<span class="o">=</span><span class="s1">&#39;*.&#39;</span><span class="o">{</span>txt,pdf,ALL<span class="o">}</span><span class="w"> </span>--exclude<span class="o">=</span><span class="s2">&quot;*&quot;</span><span class="w"> </span>aleph.gutenberg.org::gutenberg<span class="w"> </span>./gutenberg_mirror
</pre></div>
</div>
<p>It’s not mandatory to include <code class="docutils literal notranslate"><span class="pre">pdf</span></code>-files, since they are currently not used. Please review the <code class="docutils literal notranslate"><span class="pre">--dry-run</span></code> flag.</p>
<p>Once a mirror of at least all of Gutenberg’s <code class="docutils literal notranslate"><span class="pre">*.txt</span></code> files and of index-file <code class="docutils literal notranslate"><span class="pre">GUTINDEX.ALL</span></code> has been generated, it can be used via:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ml_indie_tools.Gutenberg_Dataset</span> <span class="kn">import</span> <span class="n">Gutenberg_Dataset</span>
<span class="n">gd</span> <span class="o">=</span> <span class="n">Gutenberg_Dataset</span><span class="p">(</span><span class="n">root_url</span><span class="o">=</span><span class="s1">&#39;./gutenberg_mirror&#39;</span><span class="p">)</span>  <span class="c1"># Assuming this is the file-path to the mirror image</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ml_indie_tools.Gutenberg_Dataset</span> <span class="kn">import</span> <span class="n">Gutenberg_Dataset</span>
<span class="n">gd</span> <span class="o">=</span> <span class="n">Gutenberg_Dataset</span><span class="p">()</span>  <span class="c1"># the default Gutenberg site is used. Alternative specify a specific mirror with `root_url=http://...`.</span>
</pre></div>
</div>
<p>After using one of the two methods to instantiate the <code class="docutils literal notranslate"><span class="pre">gd</span></code> object:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gd</span><span class="o">.</span><span class="n">load_index</span><span class="p">()</span>  <span class="c1"># load the index of books</span>
</pre></div>
</div>
<p>Then get a list of books (array). Each entry is a dict with meta-data:
<code class="docutils literal notranslate"><span class="pre">search_result</span></code> is a list of dictionaries containing meta-data without the actual book-text.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">search_result</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">search</span><span class="p">({</span><span class="s1">&#39;author&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;kant&#39;</span><span class="p">,</span> <span class="s1">&#39;goethe&#39;</span><span class="p">],</span> <span class="s1">&#39;language&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;german&#39;</span><span class="p">,</span> <span class="s1">&#39;english&#39;</span><span class="p">]})</span>
</pre></div>
</div>
<p>Insert the actual book text into the dictionaries. Note that download count is <a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#Gutenberg_Dataset.Gutenberg_Dataset.insert_book_texts">limited</a> if using a remote server.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">search_result</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">insert_book_texts</span><span class="p">(</span><span class="n">search_result</span><span class="p">)</span>
<span class="c1"># search_result entries now contain an additional field `text` with the filtered text of the book.</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">search_result</span><span class="p">)</span>  <span class="c1"># Display results as Pandas DataFrame</span>
<span class="n">df</span>
</pre></div>
</div>
<p>See the <a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#module-Gutenberg_Dataset">Gutenberg_Dataset API documentation</a> for details.</p>
<p>A library for character, word, or dynamical ngram tokenization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">ml_indie_tools.Gutenberg_Dataset</span> <span class="kn">import</span> <span class="n">Gutenberg_Dataset</span>
<span class="kn">from</span> <span class="nn">ml_indie_tools.Text_Dataset</span> <span class="kn">import</span> <span class="n">Text_Dataset</span>

<span class="n">gd</span><span class="o">=</span><span class="n">Gutenberg_Dataset</span><span class="p">()</span>
<span class="n">gd</span><span class="o">.</span><span class="n">load_index</span><span class="p">()</span>
<span class="n">bl</span><span class="o">=</span><span class="n">gd</span><span class="o">.</span><span class="n">search</span><span class="p">({</span><span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;proleg&#39;</span><span class="p">,</span> <span class="s1">&#39;hermen&#39;</span><span class="p">],</span> <span class="s1">&#39;language&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;english&#39;</span><span class="p">]})</span>
<span class="n">bl</span><span class="o">=</span><span class="n">gd</span><span class="o">.</span><span class="n">insert_book_texts</span><span class="p">(</span><span class="n">bl</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bl</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">bl</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;title&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>Prolegomena to the Study of Hegel’s Philosophy<span class="raw-html-m2r"><br></span>
Kant’s Prolegomena<span class="raw-html-m2r"><br></span>
The Cornish Fishermen’s Watch Night and Other Stories<span class="raw-html-m2r"><br></span>
Prolegomena to the History of Israel<span class="raw-html-m2r"><br></span>
Legge Prolegomena<span class="raw-html-m2r"><br></span></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tl</span> <span class="o">=</span> <span class="n">Text_Dataset</span><span class="p">(</span><span class="n">bl</span><span class="p">)</span>  <span class="c1"># bl contains a list of texts (books from Gutenberg)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">source_highlight</span><span class="p">(</span><span class="s2">&quot;If we write anything that contains parts of the sources, like: that is their motto, then a highlight will be applied.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">INFO:Datasets:Loaded</span> <span class="pre">5</span> <span class="pre">texts</span></code><span class="raw-html-m2r"><br></span>
If we writ<strong>e anything t</strong>[4]<strong>hat contains</strong>[1] <strong>parts of the s</strong>[4]ources, like: <strong>that is t</strong>[1]<strong>heir motto</strong>[4], then a highligh<strong>t will be a</strong>[1]pplied.<span class="raw-html-m2r"><br></span>
Sources: Julius Wellhausen: Prolegomena to the History of Israel[4], William Wallace and G. W. F. Hegel: Prolegomena to the Study of Hegel’s Philosophy[1]</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">test_text</span><span class="o">=</span><span class="s2">&quot;That would be a valid argument if we hadn&#39;t defeated it&#39;s assumptions way before.&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Text length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_text</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">test_text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">=</span><span class="s1">&#39;ngram&#39;</span>
<span class="n">tl</span><span class="o">.</span><span class="n">init_tokenizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="n">st</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">test_text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Token-count: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">st</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">st</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Text</span> <span class="pre">length</span> <span class="pre">81,</span> <span class="pre">That</span> <span class="pre">would</span> <span class="pre">be</span> <span class="pre">a</span> <span class="pre">valid</span> <span class="pre">argument</span> <span class="pre">if</span> <span class="pre">we</span> <span class="pre">hadn't</span> <span class="pre">defeated</span> <span class="pre">it's</span> <span class="pre">assumptions</span> <span class="pre">way</span> <span class="pre">before.</span>
<span class="pre">Token-count:</span> <span class="pre">27,</span> <span class="pre">[1447,</span> <span class="pre">3688,</span> <span class="pre">1722,</span> <span class="pre">4711,</span> <span class="pre">4880,</span> <span class="pre">1210,</span> <span class="pre">1393,</span> <span class="pre">4393,</span> <span class="pre">2382,</span> <span class="pre">1352,</span> <span class="pre">3655,</span> <span class="pre">1972,</span> <span class="pre">1939,</span> <span class="pre">44,</span> <span class="pre">23,</span> <span class="pre">3333,</span> <span class="pre">1871,</span> <span class="pre">4975,</span> <span class="pre">2967,</span> <span class="pre">2884,</span> <span class="pre">2216,</span> <span class="pre">2382,</span> <span class="pre">3048,</span> <span class="pre">1546,</span> <span class="pre">4589,</span> <span class="pre">2272,</span> <span class="pre">30]</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">test2</span><span class="o">=</span><span class="s2">&quot;ðƒ &quot;</span><span class="o">+</span><span class="n">test_text</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Text length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test2</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">test2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">el</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">test2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Token-count: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">el</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">el</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Text</span> <span class="pre">length</span> <span class="pre">84,</span> <span class="pre">ðƒ</span> <span class="pre">That</span> <span class="pre">would</span> <span class="pre">be</span> <span class="pre">a</span> <span class="pre">valid</span> <span class="pre">argument</span> <span class="pre">if</span> <span class="pre">we</span> <span class="pre">hadn't</span> <span class="pre">defeated</span> <span class="pre">it's</span> <span class="pre">assumptions</span> <span class="pre">way</span> <span class="pre">before.</span>
<span class="pre">Token-count:</span> <span class="pre">29,</span> <span class="pre">['&lt;unk&gt;',</span> <span class="pre">'&lt;unk&gt;',</span> <span class="pre">1397,</span> <span class="pre">3688,</span> <span class="pre">1722,</span> <span class="pre">4711,</span> <span class="pre">4880,</span> <span class="pre">1210,</span> <span class="pre">1393,</span> <span class="pre">4393,</span> <span class="pre">2382,</span> <span class="pre">1352,</span> <span class="pre">3655,</span> <span class="pre">1972,</span> <span class="pre">1939,</span> <span class="pre">44,</span> <span class="pre">23,</span> <span class="pre">3333,</span> <span class="pre">1871,</span> <span class="pre">4975,</span> <span class="pre">2967,</span> <span class="pre">2884,</span> <span class="pre">2216,</span> <span class="pre">2382,</span> <span class="pre">3048,</span> <span class="pre">1546,</span> <span class="pre">4589,</span> <span class="pre">2272,</span> <span class="pre">30]</span></code></p>
<p>See the <a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#module-Text_Dataset">Text_Dataset API documentation</a> for details.</p>
<p>See the <a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#module-ALU_Dataset">ALU_Dataset API documentation</a> for details.
A sample project is at <a class="reference external" href="https://github.com/domschl/ALU_Net">ALU_Net</a></p>
<p>A collection of Keras residual- and self-attention layers</p>
<p>See the <a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#module-keras_custom_layers">keras_custom_layers API documentation</a> for details.</p>
<p>Checkout the following jupyter notebook based projects for example-usage:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/domschl/tensor-poet">tensor-poet</a></p></li>
<li><p><a class="reference external" href="https://github.com/domschl/torch-poet">torch-poet</a></p></li>
<li><p><a class="reference external" href="https://github.com/domschl/transformer-poet">transformer-poet</a></p></li>
<li><p><a class="reference external" href="https://github.com/domschl/torch-transformer-poet">torch-transformer-poet</a>, using pytorch transformers from Andrej Karpathy’s nanoGPT as implemented in <cite>``ng-video-lecture`</cite> &lt;<a class="reference external" href="https://github.com/karpathy/ng-video-lecture">https://github.com/karpathy/ng-video-lecture</a>&gt;`_</p></li>
</ul>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/domschl/ALU_Net">ALU_Net</a></p></li>
</ul>
<ul class="simple">
<li><p>(2023-04-01, 0.8.90) API changes: WIP!</p></li>
<li><p>(2023-03-31, 0.8.0) Put compression/state experiments in separate model.</p></li>
<li><p>(2023-03-30, 0.7.0) Cleanup of bottleneck mechanism to force abstraction. Dropout behave again normal
(hacks removed).</p></li>
<li><p>(2023-03-28, 0.6.0) Add <code class="docutils literal notranslate"><span class="pre">dropout&gt;1.0</span></code> paramater to MultiHeadSelfAttention (torch): replaces ‘normal’ dropout with a linear compression by 4.0/dropout. The linear layers no longer
map n -&gt; 4n -&gt; n, but n -&gt; 4n/dropout -&gt; n. This reduces the amount of information, the net can propagate, forcing compression. Sigma_compression uses different compressions rates:
max in the middle layers, and non at start end end layers, linearly interpolating between them.</p></li>
<li><p>(2023-02-01, 0.5.6) <code class="docutils literal notranslate"><span class="pre">load_checkpoint()</span></code>, optionally only load <code class="docutils literal notranslate"><span class="pre">params</span></code>. Incompatible API-change for <code class="docutils literal notranslate"><span class="pre">load-</span></code> and <code class="docutils literal notranslate"><span class="pre">save_checkpoint()</span></code> methods!</p></li>
<li><p>(2023-01-31, 0.5.4) Add <code class="docutils literal notranslate"><span class="pre">top_k</span></code> parameter to generator. Apple MPS users beware, MPS <a class="reference external" href="https://github.com/pytorch/pytorch/issues/78915">currently limits top_k to max 16</a>.</p></li>
<li><p>(2023-01-30, 0.5.3) Add <code class="docutils literal notranslate"><span class="pre">use_aliases</span></code> parameter to Folder- and Calibre datasets.</p></li>
<li><p>(2023-01-27, 0.5.2) Add <code class="docutils literal notranslate"><span class="pre">alias</span></code> field to local datasets to protect
privacy of local document names.</p></li>
<li><p>(2023-01-27, 0.5.0) Acquire training data from Calibre library (<cite>``Calibre_Dataset`</cite> &lt;<a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#Calibre_Dataset.Calibre_Dataset">https://domschl.github.io/ml-indie-tools/_build/html/index.html#Calibre_Dataset.Calibre_Dataset</a>&gt;`_), the documents must be in text format in Calibre, or get training data from a folder containing text files (<cite>``Folder_Dataset`</cite> &lt;<a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#module-Folder_Dataset">https://domschl.github.io/ml-indie-tools/_build/html/index.html#module-Folder_Dataset</a>&gt;`_). Text_Dataset can now
contain texts from Gutenberg, Calibre or a folder of text files.</p></li>
<li><p>(2023-01-26, 0.4.4) Add save/load tokenizer to Text_Dataset to enable reusing tokenizer data.</p></li>
<li><p>(2023-01-22, 0.4.3) Add temperature parameter to generator.</p></li>
<li><p>(2023-01-21, 0.4.2) Start of port of pytorch transformers from Andrej Karpathy’s nanoGPT as implemented in <cite>``ng-video-lecture`</cite> &lt;<a class="reference external" href="https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py">https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py</a>&gt;`_. Additional tests with Apple Silicon MPS and pytorch 2.0 nightly.</p></li>
<li><p>(2022-12-13, 0.4.0) The great cleanup: neither recurrence nor gated memory improved the transformer architecture, so they are removed again.</p></li>
<li><p>(2022-12-11, 0.3.17) Testversion for slightly handwavy recurrent attention</p></li>
<li><p>(2022-06-19, 0.3.1) get_random_item(index) that works with all tokenization strategies, get_unique_token_count() added.</p></li>
<li><p>(2022-06-19, 0.3.0) Breaking change in Text_Dataset <strong>get_item</strong>() behavior, old API didn’t fit with tokenization.</p></li>
<li><p>(2022-06-19, 0.2.0) Language agnostic dynamic ngram tokenizer.</p></li>
<li><p>(2022-06-07, 0.1.5) Support for pytorch nightly 1.13dev MPS, Apple Metal acceleration on Apple Silicon.</p></li>
<li><p>(2022-03-27, 0.1.4) Bugfixes to Gutenberg <code class="docutils literal notranslate"><span class="pre">search</span></code> and <code class="docutils literal notranslate"><span class="pre">load_book</span></code> and <code class="docutils literal notranslate"><span class="pre">get_book</span></code>.</p></li>
<li><p>(2022-03-15, 0.1.2) <code class="docutils literal notranslate"><span class="pre">env_tools.init()</span></code> no longer uses <code class="docutils literal notranslate"><span class="pre">tf.compat.v1.disable_eager_executition()</span></code> since there are rumors about old code-paths being used. Use <code class="docutils literal notranslate"><span class="pre">tf.function()</span></code> instead, or call with <code class="docutils literal notranslate"><span class="pre">env_tools.init(...,</span> <span class="pre">old_disable_eager=True)</span></code> which continues to use the old v1 API.</p></li>
<li><p>(2022-03-12, 0.1.0) First version for external use.</p></li>
<li><p>(2021-12-26, 0.0.x) First pre-alpha versions published for testing purposes, not ready for use.</p></li>
</ul>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="README.html">A collection of machine learning tools for low-resource research and experiments</a><ul>
<li class="toctree-l2"><a class="reference internal" href="README.html#description">Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="README.html#external-projects">External projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="README.html#history">History</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="#">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Welcome to ml-indie-tools’s documentation!</a><ul>
<li><a class="reference internal" href="#module-env_tools">MLEnv object</a></li>
<li><a class="reference internal" href="#module-tuner">MLTuner object</a></li>
<li><a class="reference internal" href="#module-Gutenberg_Dataset">Gutenberg_Dataset object</a></li>
<li><a class="reference internal" href="#module-Calibre_Dataset">Calibre_Dataset object</a></li>
<li><a class="reference internal" href="#module-Folder_Dataset">Folder_Dataset object</a></li>
<li><a class="reference internal" href="#module-Text_Dataset">Text_Dataset object</a></li>
<li><a class="reference internal" href="#module-ALU_Dataset">ALU_Dataset object</a></li>
<li><a class="reference internal" href="#residualblock-object">ResidualBlock object</a></li>
<li><a class="reference internal" href="#residualdense-object">ResidualDense object</a></li>
<li><a class="reference internal" href="#residualdensestack-object">ResidualDenseStack object</a></li>
<li><a class="reference internal" href="#parallelresidualdensestacks-object">ParallelResidualDenseStacks object</a></li>
<li><a class="reference internal" href="#selfattention-object">SelfAttention object</a></li>
<li><a class="reference internal" href="#multiheadselfattention-object">MultiHeadSelfAttention object</a></li>
<li><a class="reference internal" href="#module-pytorch_custom_layers">PositionalEncoding object</a></li>
<li><a class="reference internal" href="#selfattentionhead">SelfAttentionHead</a></li>
<li><a class="reference internal" href="#multiheadattention">MultiHeadAttention</a></li>
<li><a class="reference internal" href="#feedfoward">FeedFoward</a></li>
<li><a class="reference internal" href="#block">Block</a></li>
<li><a class="reference internal" href="#multiheadselfattention">MultiHeadSelfAttention</a></li>
<li><a class="reference internal" href="#feedforwardwithcompression">FeedForwardWithCompression</a></li>
<li><a class="reference internal" href="#feedforwardwithcompressionstate">FeedForwardWithCompressionState</a></li>
<li><a class="reference internal" href="#blockwithcompression">BlockWithCompression</a></li>
<li><a class="reference internal" href="#blockwithcompressionstate">BlockWithCompressionState</a></li>
<li><a class="reference internal" href="#blockwithcompressionnoyokeresidual">BlockWithCompressionNoYokeResidual</a></li>
<li><a class="reference internal" href="#blockwithcompressionstatenoyokeresidual">BlockWithCompressionStateNoYokeResidual</a></li>
<li><a class="reference internal" href="#multiheadselfattentionwithcompression">MultiHeadSelfAttentionWithCompression</a></li>
<li><a class="reference internal" href="#multiheadselfattentionwithcompressionstate">MultiHeadSelfAttentionWithCompressionState</a></li>
<li><a class="reference internal" href="#readme-file">Readme File</a></li>
</ul>
</li>
<li><a class="reference internal" href="#a-collection-of-machine-learning-tools-for-low-resource-research-and-experiments">A collection of machine learning tools for low-resource research and experiments</a></li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
</ul>

  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="README.html"
                          title="next chapter">A collection of machine learning tools for low-resource research and experiments</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/index.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="README.html" title="A collection of machine learning tools for low-resource research and experiments"
             >next</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">ml-indie-tools 0.3.8 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Welcome to ml-indie-tools’s documentation!</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2022, dsc.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 4.5.0.
    </div>
  </body>
</html>