

<!doctype html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>A collection of machine learning tools for low-resource research and experiments &#8212; ml-indie-tools 0.3.8 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/bizstyle.css" />
    
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/bizstyle.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Welcome to ml-indie-tools’s documentation!" href="index.html" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0" />
    <!--[if lt IE 9]>
    <script src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="index.html" title="Welcome to ml-indie-tools’s documentation!"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">ml-indie-tools 0.3.8 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">A collection of machine learning tools for low-resource research and experiments</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="a-collection-of-machine-learning-tools-for-low-resource-research-and-experiments">
<h1>A collection of machine learning tools for low-resource research and experiments<a class="headerlink" href="#a-collection-of-machine-learning-tools-for-low-resource-research-and-experiments" title="Permalink to this headline">¶</a></h1>
<a class="reference external image-reference" href="LICENSE"><img alt="License" src="http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat" /></a>
<a class="reference external image-reference" href="https://domschl.github.io/ml-indie-tools/index.html"><img alt="Docs" src="https://img.shields.io/badge/docs-stable-blue.svg" /></a>
<a class="reference external image-reference" href="https://pypi.python.org/pypi/ml-indie-tools/"><img alt="PyPI version fury.io" src="https://badge.fury.io/py/ml-indie-tools.svg" /></a>
<section id="description">
<h2>Description<a class="headerlink" href="#description" title="Permalink to this headline">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>ml-indie-tools
</pre></div>
</div>
<p>This module contains of a collection of tools useable for researchers with limited access to compute-resources and
who change between laptop, Colab-instances and local workstations with a graphics card.</p>
<p><code class="docutils literal notranslate"><span class="pre">env_tools</span></code> checks the current environment, and populates a number of flags that allow identification of run-time
environment and available accelerator hardware. For Colab instances, it provides tools to mount Google Drive for
persistent data- and model-storage.</p>
<p>The usage scenarios are:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Env</p></th>
<th class="head"><p>Tensorflow TPU</p></th>
<th class="head"><p>Tensorflow GPU</p></th>
<th class="head"><p>Pytorch TPU</p></th>
<th class="head"><p>Pytorch GPU</p></th>
<th class="head"><p>Jax TPU</p></th>
<th class="head"><p>Jax GPU</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Colab</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p>/</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>Workstation with Nvidia</p></td>
<td><p>/</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p>/</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p>/</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>Apple Silicon</p></td>
<td><p>/</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p>/</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p>/</p></td>
<td><p>/</p></td>
</tr>
</tbody>
</table>
<p>(<cite>+</cite>: supported, <cite>/</cite>: not supported)</p>
<p><code class="docutils literal notranslate"><span class="pre">Gutenberg_Dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">Text_Dataset</span></code> are NLP libraries that provide text data and can be used in conjuction
with Huggingface <a class="reference external" href="https://huggingface.co/docs/datasets/">Datasets</a> or directly with ML libraries.</p>
<p><code class="docutils literal notranslate"><span class="pre">ALU_Dataset</span></code> is a toy-dataset that allows training of integer arithmetic and logical (ALU) operations.</p>
<section id="env-tools">
<h3>env_tools<a class="headerlink" href="#env-tools" title="Permalink to this headline">¶</a></h3>
<p>A collection of tools that allow moving machine learning projects between local hardware and colab instances.</p>
<section id="examples">
<h4>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h4>
<p>Local laptop:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ml_indie_tools.env_tools</span> <span class="kn">import</span> <span class="n">MLEnv</span>
<span class="n">ml_env</span> <span class="o">=</span> <span class="n">MLEnv</span><span class="p">(</span><span class="n">platform</span><span class="o">=</span><span class="s1">&#39;tf&#39;</span><span class="p">,</span> <span class="n">accelator</span><span class="o">=</span><span class="s1">&#39;fastest&#39;</span><span class="p">)</span>
<span class="n">ml_env</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>  <span class="c1"># -&gt; &#39;OS: Darwin, Python: 3.9.9 (Conda) Tensorflow: 2.7.0, GPU: METAL&#39;</span>
<span class="n">ml_env</span><span class="o">.</span><span class="n">is_gpu</span>   <span class="c1"># -&gt; True</span>
<span class="n">ml_env</span><span class="o">.</span><span class="n">is_tensorflow</span>  <span class="c1"># -&gt; True</span>
<span class="n">ml_env</span><span class="o">.</span><span class="n">gpu_type</span>  <span class="c1"># -&gt; &#39;METAL&#39;</span>
</pre></div>
</div>
<p>Colab instance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># !pip install -U ml_indie_tools</span>
<span class="kn">from</span> <span class="nn">ml_indie_tools.env_tools</span> <span class="kn">import</span> <span class="n">MLEnv</span>
<span class="n">ml_env</span> <span class="o">=</span> <span class="n">MLEnv</span><span class="p">(</span><span class="n">platform</span><span class="o">=</span><span class="s1">&#39;tf&#39;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s1">&#39;fastest&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ml_env</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ml_env</span><span class="o">.</span><span class="n">gpu_type</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DEBUG</span><span class="p">:</span><span class="n">MLEnv</span><span class="p">:</span><span class="n">Tensorflow</span> <span class="n">version</span><span class="p">:</span> <span class="mf">2.7.0</span>
<span class="n">DEBUG</span><span class="p">:</span><span class="n">MLEnv</span><span class="p">:</span><span class="n">GPU</span> <span class="n">available</span>
<span class="n">DEBUG</span><span class="p">:</span><span class="n">MLEnv</span><span class="p">:</span><span class="n">You</span> <span class="n">are</span> <span class="n">on</span> <span class="n">a</span> <span class="n">Jupyter</span> <span class="n">instance</span><span class="o">.</span>
<span class="n">DEBUG</span><span class="p">:</span><span class="n">MLEnv</span><span class="p">:</span><span class="n">You</span> <span class="n">are</span> <span class="n">on</span> <span class="n">a</span> <span class="n">Colab</span> <span class="n">instance</span><span class="o">.</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">MLEnv</span><span class="p">:</span><span class="n">OS</span><span class="p">:</span> <span class="n">Linux</span><span class="p">,</span> <span class="n">Python</span><span class="p">:</span> <span class="mf">3.7.12</span><span class="p">,</span> <span class="n">Colab</span> <span class="n">Jupyter</span> <span class="n">Notebook</span> <span class="n">Tensorflow</span><span class="p">:</span> <span class="mf">2.7.0</span><span class="p">,</span> <span class="n">GPU</span><span class="p">:</span> <span class="n">Tesla</span> <span class="n">K80</span>
<span class="n">The</span> <span class="n">tensorboard</span> <span class="n">extension</span> <span class="ow">is</span> <span class="n">already</span> <span class="n">loaded</span><span class="o">.</span> <span class="n">To</span> <span class="n">reload</span> <span class="n">it</span><span class="p">,</span> <span class="n">use</span><span class="p">:</span>
  <span class="o">%</span><span class="n">reload_ext</span> <span class="n">tensorboard</span>
<span class="n">OS</span><span class="p">:</span> <span class="n">Linux</span><span class="p">,</span> <span class="n">Python</span><span class="p">:</span> <span class="mf">3.7.12</span><span class="p">,</span> <span class="n">Colab</span> <span class="n">Jupyter</span> <span class="n">Notebook</span> <span class="n">Tensorflow</span><span class="p">:</span> <span class="mf">2.7.0</span><span class="p">,</span> <span class="n">GPU</span><span class="p">:</span> <span class="n">Tesla</span> <span class="n">K80</span>
<span class="n">Tesla</span> <span class="n">K80</span>
</pre></div>
</div>
</section>
<section id="project-paths">
<h4>Project paths<a class="headerlink" href="#project-paths" title="Permalink to this headline">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">ml_env.init_paths('my_project',</span> <span class="pre">'my_model')</span></code> will give a list of paths that are adapted for local and colab usage</p>
<p>Local project:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ml_env</span><span class="o">.</span><span class="n">init_paths</span><span class="p">(</span><span class="s2">&quot;my_project&quot;</span><span class="p">,</span> <span class="s2">&quot;my_model&quot;</span><span class="p">)</span>
<span class="c1"># -&gt; (&#39;.&#39;, &#39;.&#39;, &#39;./model/my_model&#39;, &#39;./data&#39;, &#39;./logs&#39;)</span>
</pre></div>
</div>
<p>The list contains <code class="docutils literal notranslate"><span class="pre">&lt;root-path&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;project-path&gt;</span></code> (both are <code class="docutils literal notranslate"><span class="pre">.</span></code>, the current directory for local projects), <code class="docutils literal notranslate"><span class="pre">&lt;model-path&gt;</span></code> to save model and weights, <code class="docutils literal notranslate"><span class="pre">&lt;data-path&gt;</span></code> for
training data and <code class="docutils literal notranslate"><span class="pre">&lt;log-path&gt;</span></code> for logs.</p>
<p>Those paths (with exception of <code class="docutils literal notranslate"><span class="pre">./logs</span></code>) are moved to Google Drive for Colab instances:</p>
<p>On Google Colab:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># INFO:MLEnv:You will now be asked to authenticate Google Drive access in order to store training data (cache) and model state.</span>
<span class="c1"># INFO:MLEnv:Changes will only happen within Google Drive directory `My Drive/Colab Notebooks/&lt;project-name&gt;`.</span>
<span class="c1"># DEBUG:MLEnv:Root path: /content/drive/My Drive</span>
<span class="c1"># Mounted at /content/drive</span>
<span class="p">(</span><span class="s1">&#39;/content/drive/My Drive&#39;</span><span class="p">,</span>
 <span class="s1">&#39;/content/drive/My Drive/Colab Notebooks/my_project&#39;</span><span class="p">,</span>
 <span class="s1">&#39;/content/drive/My Drive/Colab Notebooks/my_project/model/my_model&#39;</span><span class="p">,</span>
 <span class="s1">&#39;/content/drive/My Drive/Colab Notebooks/my_project/data&#39;</span><span class="p">,</span>
 <span class="s1">&#39;./logs&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>See the <a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#module-env_tools">env_tools API documentation</a> for details.</p>
</section>
</section>
<section id="gutenberg-dataset">
<h3>Gutenberg_Dataset<a class="headerlink" href="#gutenberg-dataset" title="Permalink to this headline">¶</a></h3>
<p>Gutenberg_Dataset makes books from <a class="reference external" href="https://www.gutenberg.org">Project Gutenberg</a> available as dataset.</p>
<p>This module can either work with a local mirror of Project Gutenberg, or download files on demand.
Files that are downloaded are cached to prevent unnecessary load on Gutenberg’s servers.</p>
<section id="working-with-a-local-mirror-of-project-gutenberg">
<h4>Working with a local mirror of Project Gutenberg<a class="headerlink" href="#working-with-a-local-mirror-of-project-gutenberg" title="Permalink to this headline">¶</a></h4>
<p>If you plan to use a lot of files (hundreds or more) from Gutenberg, a local mirror might be the best
solution. Have a look at <a class="reference external" href="https://www.gutenberg.org/help/mirroring.html">Project Gutenberg’s notes on mirrors</a>.</p>
<p>A mirror image suitable for this project can be made with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>rsync<span class="w"> </span>-zarv<span class="w"> </span>--dry-run<span class="w"> </span>--prune-empty-dirs<span class="w"> </span>--del<span class="w"> </span>--include<span class="o">=</span><span class="s2">&quot;*/&quot;</span><span class="w"> </span>--include<span class="o">=</span><span class="s1">&#39;*.&#39;</span><span class="o">{</span>txt,pdf,ALL<span class="o">}</span><span class="w"> </span>--exclude<span class="o">=</span><span class="s2">&quot;*&quot;</span><span class="w"> </span>aleph.gutenberg.org::gutenberg<span class="w"> </span>./gutenberg_mirror
</pre></div>
</div>
<p>It’s not mandatory to include <code class="docutils literal notranslate"><span class="pre">pdf</span></code>-files, since they are currently not used. Please review the <code class="docutils literal notranslate"><span class="pre">--dry-run</span></code> flag.</p>
<p>Once a mirror of at least all of Gutenberg’s <code class="docutils literal notranslate"><span class="pre">*.txt</span></code> files and of index-file <code class="docutils literal notranslate"><span class="pre">GUTINDEX.ALL</span></code> has been generated, it can be used via:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ml_indie_tools.Gutenberg_Dataset</span> <span class="kn">import</span> <span class="n">Gutenberg_Dataset</span>
<span class="n">gd</span> <span class="o">=</span> <span class="n">Gutenberg_Dataset</span><span class="p">(</span><span class="n">root_url</span><span class="o">=</span><span class="s1">&#39;./gutenberg_mirror&#39;</span><span class="p">)</span>  <span class="c1"># Assuming this is the file-path to the mirror image</span>
</pre></div>
</div>
</section>
<section id="working-without-a-remote-mirror">
<h4>Working without a remote mirror<a class="headerlink" href="#working-without-a-remote-mirror" title="Permalink to this headline">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ml_indie_tools.Gutenberg_Dataset</span> <span class="kn">import</span> <span class="n">Gutenberg_Dataset</span>
<span class="n">gd</span> <span class="o">=</span> <span class="n">Gutenberg_Dataset</span><span class="p">()</span>  <span class="c1"># the default Gutenberg site is used. Alternative specify a specific mirror with `root_url=http://...`.</span>
</pre></div>
</div>
</section>
<section id="getting-gutenberg-books">
<h4>Getting Gutenberg books<a class="headerlink" href="#getting-gutenberg-books" title="Permalink to this headline">¶</a></h4>
<p>After using one of the two methods to instantiate the <code class="docutils literal notranslate"><span class="pre">gd</span></code> object:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gd</span><span class="o">.</span><span class="n">load_index</span><span class="p">()</span>  <span class="c1"># load the index of books</span>
</pre></div>
</div>
<p>Then get a list of books (array). Each entry is a dict with meta-data:
<code class="docutils literal notranslate"><span class="pre">search_result</span></code> is a list of dictionaries containing meta-data without the actual book-text.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">search_result</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">search</span><span class="p">({</span><span class="s1">&#39;author&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;kant&#39;</span><span class="p">,</span> <span class="s1">&#39;goethe&#39;</span><span class="p">],</span> <span class="s1">&#39;language&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;german&#39;</span><span class="p">,</span> <span class="s1">&#39;english&#39;</span><span class="p">]})</span>
</pre></div>
</div>
<p>Insert the actual book text into the dictionaries. Note that download count is <a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#Gutenberg_Dataset.Gutenberg_Dataset.insert_book_texts">limited</a> if using a remote server.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">search_result</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">insert_book_texts</span><span class="p">(</span><span class="n">search_result</span><span class="p">)</span>
<span class="c1"># search_result entries now contain an additional field `text` with the filtered text of the book.</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">search_result</span><span class="p">)</span>  <span class="c1"># Display results as Pandas DataFrame</span>
<span class="n">df</span>
</pre></div>
</div>
<p>See the <a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#module-Gutenberg_Dataset">Gutenberg_Dataset API documentation</a> for details.</p>
</section>
</section>
<section id="text-dataset">
<h3>Text_Dataset<a class="headerlink" href="#text-dataset" title="Permalink to this headline">¶</a></h3>
<p>A library for character, word, or dynamical ngram tokenization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">ml_indie_tools.Gutenberg_Dataset</span> <span class="kn">import</span> <span class="n">Gutenberg_Dataset</span>
<span class="kn">from</span> <span class="nn">ml_indie_tools.Text_Dataset</span> <span class="kn">import</span> <span class="n">Text_Dataset</span>

<span class="n">gd</span><span class="o">=</span><span class="n">Gutenberg_Dataset</span><span class="p">()</span>
<span class="n">gd</span><span class="o">.</span><span class="n">load_index</span><span class="p">()</span>
<span class="n">bl</span><span class="o">=</span><span class="n">gd</span><span class="o">.</span><span class="n">search</span><span class="p">({</span><span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;proleg&#39;</span><span class="p">,</span> <span class="s1">&#39;hermen&#39;</span><span class="p">],</span> <span class="s1">&#39;language&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;english&#39;</span><span class="p">]})</span>
<span class="n">bl</span><span class="o">=</span><span class="n">gd</span><span class="o">.</span><span class="n">insert_book_texts</span><span class="p">(</span><span class="n">bl</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bl</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">bl</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;title&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>Prolegomena to the Study of Hegel’s Philosophy<span class="raw-html-m2r"><br></span>
Kant’s Prolegomena<span class="raw-html-m2r"><br></span>
The Cornish Fishermen’s Watch Night and Other Stories<span class="raw-html-m2r"><br></span>
Prolegomena to the History of Israel<span class="raw-html-m2r"><br></span>
Legge Prolegomena<span class="raw-html-m2r"><br></span></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tl</span> <span class="o">=</span> <span class="n">Text_Dataset</span><span class="p">(</span><span class="n">bl</span><span class="p">)</span>  <span class="c1"># bl contains a list of texts (books from Gutenberg)</span>
<span class="n">tl</span><span class="o">.</span><span class="n">source_highlight</span><span class="p">(</span><span class="s2">&quot;If we write anything that contains parts of the sources, like: that is their motto, then a highlight will be applied.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">INFO:Datasets:Loaded</span> <span class="pre">5</span> <span class="pre">texts</span></code><span class="raw-html-m2r"><br></span>
If we writ<strong>e anything t</strong>[4]<strong>hat contains</strong>[1] <strong>parts of the s</strong>[4]ources, like: <strong>that is t</strong>[1]<strong>heir motto</strong>[4], then a highligh<strong>t will be a</strong>[1]pplied.<span class="raw-html-m2r"><br></span>
Sources: Julius Wellhausen: Prolegomena to the History of Israel[4], William Wallace and G. W. F. Hegel: Prolegomena to the Study of Hegel’s Philosophy[1]</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">test_text</span><span class="o">=</span><span class="s2">&quot;That would be a valid argument if we hadn&#39;t defeated it&#39;s assumptions way before.&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Text length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_text</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">test_text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">=</span><span class="s1">&#39;ngram&#39;</span>
<span class="n">tl</span><span class="o">.</span><span class="n">init_tokenizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="n">st</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">test_text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Token-count: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">st</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">st</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Text</span> <span class="pre">length</span> <span class="pre">81,</span> <span class="pre">That</span> <span class="pre">would</span> <span class="pre">be</span> <span class="pre">a</span> <span class="pre">valid</span> <span class="pre">argument</span> <span class="pre">if</span> <span class="pre">we</span> <span class="pre">hadn't</span> <span class="pre">defeated</span> <span class="pre">it's</span> <span class="pre">assumptions</span> <span class="pre">way</span> <span class="pre">before.</span>
<span class="pre">Token-count:</span> <span class="pre">27,</span> <span class="pre">[1447,</span> <span class="pre">3688,</span> <span class="pre">1722,</span> <span class="pre">4711,</span> <span class="pre">4880,</span> <span class="pre">1210,</span> <span class="pre">1393,</span> <span class="pre">4393,</span> <span class="pre">2382,</span> <span class="pre">1352,</span> <span class="pre">3655,</span> <span class="pre">1972,</span> <span class="pre">1939,</span> <span class="pre">44,</span> <span class="pre">23,</span> <span class="pre">3333,</span> <span class="pre">1871,</span> <span class="pre">4975,</span> <span class="pre">2967,</span> <span class="pre">2884,</span> <span class="pre">2216,</span> <span class="pre">2382,</span> <span class="pre">3048,</span> <span class="pre">1546,</span> <span class="pre">4589,</span> <span class="pre">2272,</span> <span class="pre">30]</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">test2</span><span class="o">=</span><span class="s2">&quot;ðƒ &quot;</span><span class="o">+</span><span class="n">test_text</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Text length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test2</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">test2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">el</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">test2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Token-count: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">el</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">el</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Text</span> <span class="pre">length</span> <span class="pre">84,</span> <span class="pre">ðƒ</span> <span class="pre">That</span> <span class="pre">would</span> <span class="pre">be</span> <span class="pre">a</span> <span class="pre">valid</span> <span class="pre">argument</span> <span class="pre">if</span> <span class="pre">we</span> <span class="pre">hadn't</span> <span class="pre">defeated</span> <span class="pre">it's</span> <span class="pre">assumptions</span> <span class="pre">way</span> <span class="pre">before.</span>
<span class="pre">Token-count:</span> <span class="pre">29,</span> <span class="pre">['&lt;unk&gt;',</span> <span class="pre">'&lt;unk&gt;',</span> <span class="pre">1397,</span> <span class="pre">3688,</span> <span class="pre">1722,</span> <span class="pre">4711,</span> <span class="pre">4880,</span> <span class="pre">1210,</span> <span class="pre">1393,</span> <span class="pre">4393,</span> <span class="pre">2382,</span> <span class="pre">1352,</span> <span class="pre">3655,</span> <span class="pre">1972,</span> <span class="pre">1939,</span> <span class="pre">44,</span> <span class="pre">23,</span> <span class="pre">3333,</span> <span class="pre">1871,</span> <span class="pre">4975,</span> <span class="pre">2967,</span> <span class="pre">2884,</span> <span class="pre">2216,</span> <span class="pre">2382,</span> <span class="pre">3048,</span> <span class="pre">1546,</span> <span class="pre">4589,</span> <span class="pre">2272,</span> <span class="pre">30]</span></code></p>
<p>See the <a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#module-Text_Dataset">Text_Dataset API documentation</a> for details.</p>
</section>
<section id="alu-dataset">
<h3>ALU_Dataset<a class="headerlink" href="#alu-dataset" title="Permalink to this headline">¶</a></h3>
<p>See the <a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#module-ALU_Dataset">ALU_Dataset API documentation</a> for details.
A sample project is at <a class="reference external" href="https://github.com/domschl/ALU_Net">ALU_Net</a></p>
</section>
<section id="keras-custom-layers">
<h3>keras_custom_layers<a class="headerlink" href="#keras-custom-layers" title="Permalink to this headline">¶</a></h3>
<p>A collection of Keras residual- and self-attention layers</p>
<p>See the <a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#module-keras_custom_layers">keras_custom_layers API documentation</a> for details.</p>
</section>
</section>
<section id="external-projects">
<h2>External projects<a class="headerlink" href="#external-projects" title="Permalink to this headline">¶</a></h2>
<p>Checkout the following jupyter notebook based projects for example-usage:</p>
<section id="text-generation">
<h3>Text generation<a class="headerlink" href="#text-generation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/domschl/tensor-poet">tensor-poet</a></p></li>
<li><p><a class="reference external" href="https://github.com/domschl/torch-poet">torch-poet</a></p></li>
<li><p><a class="reference external" href="https://github.com/domschl/transformer-poet">transformer-poet</a></p></li>
<li><p><a class="reference external" href="https://github.com/domschl/torch-transformer-poet">torch-transformer-poet</a>, using pytorch transformers from Andrej Karpathy’s nanoGPT as implemented in <cite>``ng-video-lecture`</cite> &lt;<a class="reference external" href="https://github.com/karpathy/ng-video-lecture">https://github.com/karpathy/ng-video-lecture</a>&gt;`_</p></li>
</ul>
</section>
<section id="arithmetic-and-logic-operations">
<h3>Arithmetic and logic operations<a class="headerlink" href="#arithmetic-and-logic-operations" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/domschl/ALU_Net">ALU_Net</a></p></li>
</ul>
</section>
</section>
<section id="history">
<h2>History<a class="headerlink" href="#history" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>(2023-04-01, 0.8.90) API changes: WIP!</p></li>
<li><p>(2023-03-31, 0.8.0) Put compression/state experiments in separate model.</p></li>
<li><p>(2023-03-30, 0.7.0) Cleanup of bottleneck mechanism to force abstraction. Dropout behave again normal
(hacks removed).</p></li>
<li><p>(2023-03-28, 0.6.0) Add <code class="docutils literal notranslate"><span class="pre">dropout&gt;1.0</span></code> paramater to MultiHeadSelfAttention (torch): replaces ‘normal’ dropout with a linear compression by 4.0/dropout. The linear layers no longer
map n -&gt; 4n -&gt; n, but n -&gt; 4n/dropout -&gt; n. This reduces the amount of information, the net can propagate, forcing compression. Sigma_compression uses different compressions rates:
max in the middle layers, and non at start end end layers, linearly interpolating between them.</p></li>
<li><p>(2023-02-01, 0.5.6) <code class="docutils literal notranslate"><span class="pre">load_checkpoint()</span></code>, optionally only load <code class="docutils literal notranslate"><span class="pre">params</span></code>. Incompatible API-change for <code class="docutils literal notranslate"><span class="pre">load-</span></code> and <code class="docutils literal notranslate"><span class="pre">save_checkpoint()</span></code> methods!</p></li>
<li><p>(2023-01-31, 0.5.4) Add <code class="docutils literal notranslate"><span class="pre">top_k</span></code> parameter to generator. Apple MPS users beware, MPS <a class="reference external" href="https://github.com/pytorch/pytorch/issues/78915">currently limits top_k to max 16</a>.</p></li>
<li><p>(2023-01-30, 0.5.3) Add <code class="docutils literal notranslate"><span class="pre">use_aliases</span></code> parameter to Folder- and Calibre datasets.</p></li>
<li><p>(2023-01-27, 0.5.2) Add <code class="docutils literal notranslate"><span class="pre">alias</span></code> field to local datasets to protect
privacy of local document names.</p></li>
<li><p>(2023-01-27, 0.5.0) Acquire training data from Calibre library (<cite>``Calibre_Dataset`</cite> &lt;<a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#Calibre_Dataset.Calibre_Dataset">https://domschl.github.io/ml-indie-tools/_build/html/index.html#Calibre_Dataset.Calibre_Dataset</a>&gt;`_), the documents must be in text format in Calibre, or get training data from a folder containing text files (<cite>``Folder_Dataset`</cite> &lt;<a class="reference external" href="https://domschl.github.io/ml-indie-tools/_build/html/index.html#module-Folder_Dataset">https://domschl.github.io/ml-indie-tools/_build/html/index.html#module-Folder_Dataset</a>&gt;`_). Text_Dataset can now
contain texts from Gutenberg, Calibre or a folder of text files.</p></li>
<li><p>(2023-01-26, 0.4.4) Add save/load tokenizer to Text_Dataset to enable reusing tokenizer data.</p></li>
<li><p>(2023-01-22, 0.4.3) Add temperature parameter to generator.</p></li>
<li><p>(2023-01-21, 0.4.2) Start of port of pytorch transformers from Andrej Karpathy’s nanoGPT as implemented in <cite>``ng-video-lecture`</cite> &lt;<a class="reference external" href="https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py">https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py</a>&gt;`_. Additional tests with Apple Silicon MPS and pytorch 2.0 nightly.</p></li>
<li><p>(2022-12-13, 0.4.0) The great cleanup: neither recurrence nor gated memory improved the transformer architecture, so they are removed again.</p></li>
<li><p>(2022-12-11, 0.3.17) Testversion for slightly handwavy recurrent attention</p></li>
<li><p>(2022-06-19, 0.3.1) get_random_item(index) that works with all tokenization strategies, get_unique_token_count() added.</p></li>
<li><p>(2022-06-19, 0.3.0) Breaking change in Text_Dataset <strong>get_item</strong>() behavior, old API didn’t fit with tokenization.</p></li>
<li><p>(2022-06-19, 0.2.0) Language agnostic dynamic ngram tokenizer.</p></li>
<li><p>(2022-06-07, 0.1.5) Support for pytorch nightly 1.13dev MPS, Apple Metal acceleration on Apple Silicon.</p></li>
<li><p>(2022-03-27, 0.1.4) Bugfixes to Gutenberg <code class="docutils literal notranslate"><span class="pre">search</span></code> and <code class="docutils literal notranslate"><span class="pre">load_book</span></code> and <code class="docutils literal notranslate"><span class="pre">get_book</span></code>.</p></li>
<li><p>(2022-03-15, 0.1.2) <code class="docutils literal notranslate"><span class="pre">env_tools.init()</span></code> no longer uses <code class="docutils literal notranslate"><span class="pre">tf.compat.v1.disable_eager_executition()</span></code> since there are rumors about old code-paths being used. Use <code class="docutils literal notranslate"><span class="pre">tf.function()</span></code> instead, or call with <code class="docutils literal notranslate"><span class="pre">env_tools.init(...,</span> <span class="pre">old_disable_eager=True)</span></code> which continues to use the old v1 API.</p></li>
<li><p>(2022-03-12, 0.1.0) First version for external use.</p></li>
<li><p>(2021-12-26, 0.0.x) First pre-alpha versions published for testing purposes, not ready for use.</p></li>
</ul>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">A collection of machine learning tools for low-resource research and experiments</a><ul>
<li><a class="reference internal" href="#description">Description</a><ul>
<li><a class="reference internal" href="#env-tools">env_tools</a><ul>
<li><a class="reference internal" href="#examples">Examples</a></li>
<li><a class="reference internal" href="#project-paths">Project paths</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gutenberg-dataset">Gutenberg_Dataset</a><ul>
<li><a class="reference internal" href="#working-with-a-local-mirror-of-project-gutenberg">Working with a local mirror of Project Gutenberg</a></li>
<li><a class="reference internal" href="#working-without-a-remote-mirror">Working without a remote mirror</a></li>
<li><a class="reference internal" href="#getting-gutenberg-books">Getting Gutenberg books</a></li>
</ul>
</li>
<li><a class="reference internal" href="#text-dataset">Text_Dataset</a></li>
<li><a class="reference internal" href="#alu-dataset">ALU_Dataset</a></li>
<li><a class="reference internal" href="#keras-custom-layers">keras_custom_layers</a></li>
</ul>
</li>
<li><a class="reference internal" href="#external-projects">External projects</a><ul>
<li><a class="reference internal" href="#text-generation">Text generation</a></li>
<li><a class="reference internal" href="#arithmetic-and-logic-operations">Arithmetic and logic operations</a></li>
</ul>
</li>
<li><a class="reference internal" href="#history">History</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="index.html"
                          title="previous chapter">Welcome to ml-indie-tools’s documentation!</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/README.md.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="index.html" title="Welcome to ml-indie-tools’s documentation!"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">ml-indie-tools 0.3.8 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">A collection of machine learning tools for low-resource research and experiments</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2022, dsc.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 4.5.0.
    </div>
  </body>
</html>